{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección automática de repeticiones parciales en repositorios digitales\n",
    "\n",
    "El repositorio del proyecto es [Github]().\n",
    "\n",
    "# TO DO\n",
    "* generell auf we umschreiben im tutorial-teil\n",
    "\n",
    "## Why to Read this Tutorial?\n",
    "* In this tutorial, you will learn to read metadata from an OAI-PMH data provider and how to convert the retrieved data from Dublin Core to a pandas data frame.\n",
    "* Furthermore, you will carry out some basic data analysis on your data in order to find out if the data is corrupt or unclean. Based on an example, you will clean some aspects of your data.\n",
    "* Finally, you will analyse and visualize the data with the help of a network graph.\n",
    "\n",
    "![title](img/architecture.png)\n",
    "\n",
    "### Preparations\n",
    "The following code snippet initializes your Python run-time enviroment in order to run all of the subsequent actions.\n",
    "\n",
    "If you have installed everything correctly with the help of [Anaconda](https://www.continuum.io/downloads) as explained in [dst4l0.ipynb](https://github.com/elektrobohemian/dst4l-copenhagen/blob/master/dst4l0.ipynb), the following packages should be installed correctly and be importable without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jellyfish\n",
    "!pip install geojson\n",
    "!pip install geopy\n",
    "#Sickle is a lightweight OAI-PMH client library written in Python. It has been designed for conveniently retrieving data \n",
    "#from OAI interfaces the Pythonic way\n",
    "!pip install sickle\n",
    "!pip install googlemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Configuration of This Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If you set 'allowDownloads' to True, all images will be downloaded and processed again\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "allowDownloads=True # should be True if you run this for the first time\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# Download directory used for images etc.\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "downloadDir=\"./tmp/\"\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# Keep original downloaded TIFF images? Otherwise, they will be deleted and only JPEG thumbnails remain after download.\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "\n",
    "keepTIFFs=False\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If you set 'demoClustering' to False, the clustering steps will take about 2 hours\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "demoClustering=False # should be False if you run this for the first time\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If you set 'allowComputationallyExpensiveCalculations' to True, various time consuming tasks (some of them running \n",
    "# several hours) will be carried out and not only loaded from pre-computed data files\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "allowComputationallyExpensiveCalculations=True # should be True if you run this for the first time\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# Enabling this action activates exact string matching againt name lists\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "allowExactStringMatching=True\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If you set to True, some plots will be saved as PDF documents in ./figures/\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "saveFiguresAsPDF=True\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If you set to True, Google Map API will be used to resolve location names, if False OpenStreetMap will be used\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *i\n",
    "useGoogleMapsAPI=False\n",
    "privateGoogleKey='ENTER YOUR KEY HERE' #if you want to use Google, you need a personal key\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If set to True, OpenStreetMap API will be used to fetch spatial names and alternative localized names, should be True at first run\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "getSpatialNamesFromOSM=False\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If set to True, details per PPN will be saved as separate JSON files in \"jsonWebDir\"\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "serializePPNLookup2JSON=False\n",
    "jsonWebDir=\"./web/data/ppn/\"\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If set to True, the raw features created by the Java application will be read from disk and converted for further usage, must be True at first run\n",
    "#\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "reinterpretVisualWordRawFeatures=False\n",
    "\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "#\n",
    "# If set to True, a HTTP webserver will be started after all cells have been processed to show the results\n",
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "launchHTTPServer=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict # provides the ordered dictionary\n",
    "import re # for regular expressions used below\n",
    "import urllib # to read from URLs\n",
    "import json\n",
    "import networkx as nx # network analysis\n",
    "from networkx.readwrite import json_graph\n",
    "import itertools\n",
    "import os.path\n",
    "from datetime import datetime # for time measurement\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import subprocess as subp\n",
    "import gzip\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "from jellyfish import jaro_distance, jaro_winkler, hamming_distance, levenshtein_distance\n",
    "import scipy.cluster.hierarchy as scipycluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from skimage import io, exposure\n",
    "from scipy.spatial import distance\n",
    "# import the k-means algorithm\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin,pairwise_distances_argmin_min, pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# OAI\n",
    "from sickle import Sickle\n",
    "\n",
    "# image handling\n",
    "from PIL import Image\n",
    "\n",
    "# geo stuff\n",
    "import googlemaps\n",
    "from geopy.distance import vincenty\n",
    "import geojson as gj\n",
    "\n",
    "def printLog(text):\n",
    "    now=str(datetime.now())\n",
    "    print(\"[\"+now+\"]\\t\"+text)\n",
    "    # forces to output the result of the print command immediately, see: http://stackoverflow.com/questions/230751/how-to-flush-output-of-python-print\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def pickleCompress(fileName,pickledObject):\n",
    "    printLog(\"Pickling to '%s'\" %fileName)\n",
    "    f = gzip.open(fileName,'wb')\n",
    "    pickle.dump(pickledObject,f)\n",
    "    f.close()\n",
    "    printLog(\"Pickling done.\")\n",
    "    \n",
    "def pickleDecompress(fileName):\n",
    "    #restore the object\n",
    "    printLog(\"Depickling from '%s'\" %fileName)\n",
    "    f = gzip.open(fileName,'rb')\n",
    "    pickledObject = pickle.load(f)\n",
    "    f.close()\n",
    "    printLog(\"Depickling done.\")\n",
    "    return pickledObject\n",
    "\n",
    "if not os.path.exists(downloadDir):\n",
    "        os.makedirs(downloadDir)\n",
    "if not os.path.exists(\"./picklez/\"):\n",
    "        os.makedirs(\"./picklez/\")\n",
    "if not os.path.exists(\"./figures/\"):\n",
    "        os.makedirs(\"./figures/\")\n",
    "if not os.path.exists(\"./graphs/\"):\n",
    "        os.makedirs(\"./graphs/\")\n",
    "if not os.path.exists(\"./graphs/generics/\"):\n",
    "        os.makedirs(\"./graphs/generics/\")        \n",
    "if not os.path.exists(\"./html/\"):\n",
    "        os.makedirs(\"./html/\")\n",
    "if not os.path.exists(\"./web/\"):\n",
    "        os.makedirs(\"./web/\")\n",
    "if not os.path.exists(\"./web/qa/\"):\n",
    "        os.makedirs(\"./web/qa/\")\n",
    "if not os.path.exists(\"./web/data/\"):\n",
    "        os.makedirs(\"./web/data/\")\n",
    "if not os.path.exists(\"./web/data/ppn/\"):\n",
    "        os.makedirs(\"./web/data/ppn/\")\n",
    "if not os.path.exists(\"./web/data/layers/\"):\n",
    "        os.makedirs(\"./web/data/layers/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only missing package is most likely Jellyfish, which provides support for string matching and offers such phonetic distance functions that we will need below. For further details, see the [Jellyfish homepage](https://pypi.python.org/pypi/jellyfish). The package can be installed by running the following command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Data\n",
    "### Connecting to the OAI-conform Repository and Downloading Metadata Records\n",
    "\n",
    "The next cell connects to the Berlin State Library OAI-PMH server to download metadata records in the [Dublin Core format](http://dublincore.org/).\n",
    "The records are then saved locally to work with them later. For the sake of simplicity, the saving relies on the pickling mechanism of Python - its built-in object serialization method. In a productive use case, you might reach the limits of the pickle package because of the resulting file size. A viable alternative is [HDF5](https://www.h5py.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_address = 'http://sedici.unlp.edu.ar/oai/snrd'\n",
    "#repository_address = 'http://repositorio.filo.uba.ar/oai/request'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-11 00:18:01.238000]\tStarting OAI-PMH record download...\n",
      "[2020-08-11 00:18:01.394000]\tFinished OAI-PMH download of 100 records.\n"
     ]
    }
   ],
   "source": [
    "savedRecords=[]\n",
    "if allowDownloads:\n",
    "    # connect to a metadata repository\n",
    "    #sickle = Sickle('http://digital.staatsbibliothek-berlin.de/oai')\n",
    "    sickle = Sickle(repository_address)\n",
    "    # for debugging purposes you might want to uncomment the following commands\n",
    "    # get the sets from the data provider connected to\n",
    "    #sets = sickle.ListSets()\n",
    "    #print the returned sets including their identifiers\n",
    "    #print(\"Sets provided by data provider\\n* * * * * * * * * * * * * * * * * * * * * \") # \\n creates a new line\n",
    "    #for s in sets:\n",
    "        #print(\"'\"+s.setName+\"' accessible via: '\"+s.setSpec+\"'\")\n",
    "\n",
    "    # get the records from this repository's specific document set 'DC_krieg.1914.1918' (documents related to World War I) \n",
    "    # using Dublin Core format \n",
    "    #records = sickle.ListRecords(metadataPrefix='oai_dc', set='DC_allf')\n",
    "    records = sickle.ListRecords(metadataPrefix='oai_dc')\n",
    "    printLog(\"Starting OAI-PMH record download...\")\n",
    "    # initialize some variables for counting and saving the metadata records\n",
    "    savedDocs=0\n",
    "    # 2:15 h for 100k\n",
    "    maxDocs=100 # 100 is just for testing, for more interesting results increase this value to 1000. ATTENTION! this will also take more time for reading data.\n",
    "\n",
    "    # save the records locally as we don't want to have to rely on a connection to the OAI-PMH server all the time\n",
    "    # iterate over all records until maxDocs is reached\n",
    "    # ATTENTION! if you re-run this cell, the contents of the savedRecords array will be altered!\n",
    "    for record in records:\n",
    "        # get the PPN of the digitized work as the DC identifier will reference different (analog) manifestations\n",
    "        tokens=record.header.identifier.split(\":\")\n",
    "        tokens\n",
    "        for t in tokens:\n",
    "            if t.startswith(\"PPN\"):\n",
    "                # the array building simply fixes the issue that all DC field come as arrays\n",
    "                record.metadata['PPN']=[t]\n",
    "        \n",
    "        # check if we reach the maximum document value\n",
    "        if savedDocs<maxDocs:\n",
    "            savedDocs=savedDocs+1\n",
    "            # save the current record to the \"savedRecords\" array\n",
    "            savedRecords.append(record.metadata)\n",
    "            if savedDocs%1000==0:\n",
    "                printLog(\"Downloaded %d of %d records.\"%(savedDocs,maxDocs))\n",
    "        # if so, end the processing of the for-loop\n",
    "        else:\n",
    "            break # break ends the processing of the loop\n",
    "\n",
    "    printLog(\"Finished OAI-PMH download of \"+str(len(savedRecords))+\" records.\")\n",
    "    pickle.dump( savedRecords, open( \"save_120k_dc_all.pickle\", \"wb\" ) )\n",
    "else:\n",
    "    printLog(\"Loading OAI-PMH records from disk...\")\n",
    "    savedRecords=pickle.load( open( \"save_120k_dc_all.pickle\", \"rb\" ) )\n",
    "    printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://sedici.unlp.edu.ar/handle/10915/1393',\n",
       " 'https://doi.org/10.35537/10915/1393',\n",
       " 'http://digital.cic.gba.gob.ar/handle/11746/3445']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = savedRecords[34]\n",
    "record[\"identifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://sedici.unlp.edu.ar/bitstream/handle/10915/1377/Documento_completo.pdf?sequence=1&isAllowed=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributor': ['Christiansen, Carlos'],\n",
       " 'creator': ['Cancelo, Gustavo Indalecio Eugenio'],\n",
       " 'date': ['1996'],\n",
       " 'description': [u'En esta tesis se realiza un estudio sobre las posibilidades de la electr\\xf3nica para modelar unidades y sistemas neuronales. En particular, se proponen implementaciones de redes neuronales anal\\xf3gicas. La primera a nivel microelectr\\xf3nico, comprende el dise\\xf1o de una neurona computacional con funci\\xf3n de activaci\\xf3n gaussiana. Primeramente, se demostrar\\xe1 que las unidades computacionales con funci\\xf3n de activaci\\xf3n gaussiana forman una base para aproximadores universales de funciones continuas y discontinuas medibles. La segunda abarca la realizaci\\xf3n de una neurocomputadora basada en un dispositivo neuronal anal\\xf3gico. Este sistema aprovecha toda la potencia de c\\xf3mputo de un procesador neuronal capaz de procesar 300 mil patrones anal\\xf3gicos por segundo. El ambiente desarrollado permite que dicho procesador se convierta en el coraz\\xf3n de una computadora neuronal, dentro de una computadora personal tradicional. Para ello. se desarrolla el hardware de interfaz y procesamiento necesario y el software para comunicaci\\xf3n entre procesadores, conversi\\xf3n an\\xe1logo-digital de patrones y de manejo del sistema computador.',\n",
       "  u'Tesis digitalizada en SEDICI gracias a la colaboraci\\xf3n de la Biblioteca de la Facultad de Ingenier\\xeda (UNLP).',\n",
       "  u'Facultad de Ingenier\\xeda'],\n",
       " 'format': ['application/pdf'],\n",
       " 'identifier': ['http://sedici.unlp.edu.ar/handle/10915/1374',\n",
       "  'https://doi.org/10.35537/10915/1374'],\n",
       " 'language': ['spa'],\n",
       " 'rights': ['info:eu-repo/semantics/openAccess',\n",
       "  'http://creativecommons.org/licenses/by-nc/4.0/',\n",
       "  'Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)'],\n",
       " 'subject': [u'Ingenier\\xeda'],\n",
       " 'title': [u'Redes neuronales anal\\xf3gicas para la computaci\\xf3n paralela masiva de se\\xf1ales en tiempo real'],\n",
       " 'type': ['info:eu-repo/semantics/doctoralThesis',\n",
       "  'info:ar-repo/semantics/tesis doctoral',\n",
       "  'info:eu-repo/semantics/acceptedVersion',\n",
       "  'Tesis de doctorado']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uncomment this cell if you want to split the image download to two different computers\n",
    "# in this case, one computer will use \"even_dc_all.pickle\" while the other will use \"odd_dc_all.pickle\" in the next cell\n",
    "\n",
    "#availableKeys=dict()\n",
    "#evenRecords=[]\n",
    "#oddRecords=[]\n",
    "\n",
    "#for i,r in enumerate(savedRecords):\n",
    "#    for k in r.keys():\n",
    "#        if not k in availableKeys:\n",
    "#            availableKeys[k]=1\n",
    "#        else:\n",
    "#            availableKeys[k]=availableKeys[k]+1\n",
    "#    if i%2==0:\n",
    "#        evenRecords.append(r)\n",
    "#    else:\n",
    "#        oddRecords.append(r)\n",
    "\n",
    "#pickle.dump( evenRecords, open( \"even_dc_all.pickle\", \"wb\" ) )\n",
    "#pickle.dump( oddRecords, open( \"odd_dc_all.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, all images are downloaded, downscaled and converted to the JPEG format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment this line if you are continuing the execution of this notebook at a later point in time\n",
    "#savedRecords=pickle.load( open( \"save_120k_dc_all.pickle\", \"rb\" ) )\n",
    "tiffDownloadPath=\"https://ngcs.staatsbibliothek-berlin.de/?action=metsImage&format=jpg&metsFile=%PPN%&divID=PHYS_0001&original=true\"\n",
    "countSavedRecords=len(savedRecords)\n",
    "printLog(\"Started image download and processing. This will take a while...\")\n",
    "#logFile = open(\"/Volumes/2TB_WD/sbb_images/downloadIssues.txt\", \"w\")\n",
    "logFile = open(downloadDir+\"/downloadIssues.txt\", \"w\")\n",
    "\n",
    "# surpress PIL's DecompressionBombErrors and Warnings\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "for i,record in enumerate(savedRecords):\n",
    "    if i%1000==0:\n",
    "        if allowDownloads:\n",
    "            printLog(\"Downloaded %d of %d images.\"%(i,countSavedRecords))\n",
    "    \n",
    "    ppn=\"\"\n",
    "    for r in record[\"identifier\"]:\n",
    "        if r.startswith(\"PPN\"):\n",
    "            ppn=r\n",
    "            break\n",
    "    #if len(record[\"identifier\"])>1:\n",
    "    #    ppn=str(record[\"identifier\"][1])\n",
    "    #else:\n",
    "    #    ppn=str(record[\"identifier\"][0])\n",
    "    ppnTIFF=ppn+\".tif\"\n",
    "    ppnJPEGPAth=downloadDir+ppn+\".jpg\"\n",
    "    #if \"object\" in record.keys() and allowDownloads:\n",
    "    if allowDownloads:\n",
    "        httpCode=200\n",
    "        # prevent downloading of already present files\n",
    "        if not os.path.isfile(ppnJPEGPAth) :\n",
    "            tryDownload=True\n",
    "            # check for the HTTP error code, maybe the file does not exist\n",
    "            try:\n",
    "                response=urllib.request.urlopen(tiffDownloadPath.replace(\"%PPN%\",ppn))\n",
    "            except urllib.error.HTTPError as ex:\n",
    "                #print(\"Error with %s\"%tiffDownloadPath.replace(\"%PPN%\",ppn))\n",
    "                httpCode=ex.code\n",
    "                tryDownload=False\n",
    "            if tryDownload:\n",
    "                #if allowDownloads:\n",
    "                urlinfo=urllib.request.urlretrieve(tiffDownloadPath.replace(\"%PPN%\",ppn),downloadDir+ppnTIFF)\n",
    "                img = Image.open(downloadDir+ppnTIFF)\n",
    "                img.thumbnail((512,512))\n",
    "                img.save(downloadDir+ppn+\".jpg\")\n",
    "                if not keepTIFFs:\n",
    "                    os.remove(downloadDir+ppnTIFF)\n",
    "                #ret=subp.call([\"mogrify\", \"-resize\",\"512x512\",\"-format\", \"jpg\",downloadDir+ppnTIFF])\n",
    "                #if ret!=0:\n",
    "                #    print(\"Problem with mogrifying \"+ppnTIFF)\n",
    "                #    logFile.write(\"[MOGRIFY]: %s \\n%s\\n\\n\" % (str(\"Problem with mogrifying \"+ppnTIFF),str(\"Downloaded from: \"+record[\"object\"][0])))\n",
    "                #ret=subp.call([\"rm\",downloadDir+ppnTIFF])\n",
    "                #if ret!=0:\n",
    "                #    print(\"Problem with removing \"+ppnTIFF)\n",
    "                #    logFile.write(\"[REMOVAL]: %s\\n\\n\" % \"Problem with removing \"+ppnTIFF)\n",
    "            else:\n",
    "                print(\"Problem with accessing \"+ppnTIFF+ \" @ \"+tiffDownloadPath.replace(\"%PPN%\",ppn)+\" due to HTTP code: \"+str(httpCode))\n",
    "                logFile.write(\"[HTTP]: %s\\n\\n\" % \"Problem with accessing \"+tiffDownloadPath.replace(\"%PPN%\",ppn))\n",
    "                logFile.write(\"\\tHTTP Code: \"+str(httpCode)+\"\\n\")\n",
    "                #logFile.write(str(urlinfo[1])+\"\\n\\n\")\n",
    "    else:\n",
    "        logFile.write(\"[OBJECT key missing]: %s\\n\\n\" % str(record))\n",
    "logFile.close()\n",
    "print(\"\\n\")\n",
    "if allowDownloads:\n",
    "    printLog(\"Finished image download and processing.\")\n",
    "else:\n",
    "    printLog(\"Skipped image download and processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that images will not be downloaded for some metadata records. This is no error because periodicals or multivolume work do not come with presentation images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataframe from the Metadata Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the records\n",
    "printLog(\"Loading pickled records...\")\n",
    "# uncomment this line if you are continuing the execution of this notebook at a later point in time\n",
    "savedRecords=pickle.load( open( \"save_120k_dc_all.pickle\", \"rb\" ) )\n",
    "printLog(\"Finished loading pickled records.\")\n",
    "\n",
    "availableKeys=dict()\n",
    "\n",
    "# check for all keys present in the previously downloaded dataset\n",
    "for i,r in enumerate(savedRecords):\n",
    "    for k in r.keys():\n",
    "        if not k in availableKeys:\n",
    "            availableKeys[k]=1\n",
    "        else:\n",
    "            availableKeys[k]=availableKeys[k]+1\n",
    "    \n",
    "print(availableKeys)\n",
    "\n",
    "# create a dictionary for the records\n",
    "values=dict()\n",
    "# take the keys as they have found within the downloaded OAI records\n",
    "keys=availableKeys.keys()\n",
    "# for every metadata field, create an empty array as the content of the dictionary filed under the key 'k'\n",
    "for k in keys:\n",
    "    values[k]=[]\n",
    "# in addition, store the PPN (the SBB's unique identifier for digitized content)    \n",
    "#values[\"PPN\"]=[]\n",
    "\n",
    "# iterate over all saved records\n",
    "for record in savedRecords:\n",
    "    # we cannot iterate over the keys of record.metadata directly because not all records cotain the same fields,...\n",
    "    for k in keys:\n",
    "        # thus we check if the metadata field 'k' has been created above\n",
    "        if k in values:\n",
    "            # append the metadata fields to the dictionary created above\n",
    "            # if the metadata field 'k' is not available input \"None\" instead\n",
    "            #values[k].append(record.get(k,[\"None\"])[0].encode('ISO-8859-1'))\n",
    "            if k in record:\n",
    "                value=record.get(k)[0]\n",
    "                if value:\n",
    "                    if value.isdigit():\n",
    "                        value=int(value)\n",
    "                    else:\n",
    "                        #p27 value=value.encode('ISO-8859-1')\n",
    "                            #value=value.encode('ISO-8859-1').decode(\"utf-8\", \"backslashreplace\")\n",
    "                        value=value\n",
    "                    values[k].append(value)\n",
    "                else:\n",
    "                    values[k].append(np.nan)\n",
    "            else:\n",
    "                values[k].append(np.nan)\n",
    "# create a data frame from the \n",
    "#p27 df=pd.DataFrame(pd.to_numeric(values,errors='coerce'))\n",
    "df=pd.DataFrame(values)\n",
    "df['date']=pd.to_numeric(df['date'],errors='ignore',downcast='integer')\n",
    "#df=pd.DataFrame(values)\n",
    "#df=df.convert_objects(convert_dates=False, convert_numeric=True, convert_timedeltas=False, copy=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a PPN list in CSV format is handy if you want to use it with sbbget from StabiHacks\n",
    "#df.PPN.to_csv(\"120k_ppn_list.csv\",index=False)\n",
    "\n",
    "# in case you want to use this data frame somewhere else, here are some sample serializations\n",
    "#printLog(\"Serializing data frame...\")\n",
    "# Microsoft Excel\n",
    "#df.to_excel(\"120k_sbb_sample.xlsx\")\n",
    "# HDF5 format\n",
    "#df.to_hdf(\"120k_sbb_sample.hdf5\",key=\"sbb\")\n",
    "#printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to continue from here, it might be a good idea to \n",
    "df=pd.read_excel(\"ppn_records_146000.xlsx\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.PPN.isnull()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we know that some columns should only contain unique values, e.g., the PPN column. Hence, it is a good starting point for the analysis to validate this hypothesis with the help of the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checks for unique values in each column\n",
    "def uniqueValues(currentDataFrame):\n",
    "    colNames=currentDataFrame.columns.values.tolist()\n",
    "    for colName in colNames:\n",
    "        print(colName+\";\\t\\t unique values:\\t\"+str(len(currentDataFrame[colName].unique()))+ \"\\t total count: \"+str(currentDataFrame[colName].count()))\n",
    "\n",
    "uniqueValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all entries in the PPN column are unique. In other works, some works appear more than one time in the retrieved dataset. This is an observation we should definitely take care of later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.maxmind.com/en/free-world-cities-database\n",
    "* http://www.geonames.org/export/\n",
    "* http://www.opengeocode.org/download.php#cities\n",
    "* https://en.wikipedia.org/wiki/Lists_of_cities_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Data with the Help of Regular Expressions\n",
    "\n",
    "As our last observation made us sceptical about the general data quality, we should analyse the data in more detail. Regular expressions are a handy means in order to analyse columns that we expect to follow a certain internal structure or pattern.\n",
    "\n",
    "The next cell creates various regular expression to discover positive or negative numbers, different date ranges, emails, ISBN numbers or the like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expressions taken from: http://stackoverflow.com/questions/1449817/what-are-some-of-the-most-useful-regular-expressions-for-programmers\n",
    "# extended by David Zellhöfer\n",
    "\n",
    "patterns=dict()\n",
    "#^ beginning of string, $ end of string, \\d digits +: once or more times\n",
    "patterns[\"positiveInteger\"]=\"^\\d+$\"\n",
    "patterns[\"negativeInteger\"]=\"^-\\d+$\"\n",
    "patterns[\"generalInteger\"]=\"^-?\\d+$\"\n",
    "patterns[\"positiveFloat\"]=\"^\\d*\\.\\d+$\"\n",
    "patterns[\"negativeFloat\"]=\"^-\\d*\\.\\d+$\"\n",
    "patterns[\"generalFloat\"]=\"^-?\\d*\\.\\d+$\"\n",
    "patterns[\"positiveGermanFloat\"]=\"^\\d*,\\d+$\"\n",
    "patterns[\"negativeGermanFloat\"]=\"^-\\d*,\\d+$\"\n",
    "patterns[\"generalGermanFloat\"]=\"^-?\\d*,\\d+$\"\n",
    "# Date (dd mm yyyy, d/m/yyyy, etc.), in range 1000-2099 without proper February handling\n",
    "patterns[\"dateVariant\"]=\"^([1-9]|0[1-9]|[12][0-9]|3[01])\\D([1-9]|0[1-9]|1[012])\\D(1[0-9][0-9][0-9]|20[0-9][0-9])$\"\n",
    "patterns[\"year\"]=\"^(1[0-9][0-9][0-9]|20[0-9][0-9])$\"\n",
    "patterns[\"ancientYear\"]=\"^([0-1]?[0-9][0-9][0-9]|20[0-9][0-9])$\"\n",
    "patterns[\"century\"]=\"^(1[0-9][Xx][Xx]|20[Xx][Xx])$\"\n",
    "patterns[\"ancientCentury\"]=\"^([0-1]?[0-9][Xx][Xx]|20[Xx][Xx])$\"\n",
    "patterns[\"decade\"]=\"^(1[0-9][0-9][Xx]|20[0-9][Xx])$\"\n",
    "patterns[\"ancientDecade\"]=\"^([0-1]?[0-9][0-9][Xx]|20[0-9][Xx])$\"\n",
    "# year range with splitter \"- / :\", the splitter can be surrounded by an arbitrary amount of whitespaces (\\s)\n",
    "patterns[\"rangeYear\"]=\"^\\s*(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*(\\-|\\/|:)\\s*(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*$\"\n",
    "patterns[\"rangeCentury\"]=\"^\\s*(1[0-9][Xx][Xx]|20[Xx][Xx])\\s*(\\-|\\/|:)\\s*(1[0-9][Xx][Xx]|20[Xx][Xx])\\s*$\"\n",
    "patterns[\"rangeAncientYear\"]=\"^\\s*([0-1]?[0-9][0-9][0-9]|20[0-9][0-9])\\s*(\\-|\\/|:)\\s*(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*$\"\n",
    "patterns[\"rangeAncientCentury\"]=\"^\\s*([0-1]?[0-9][Xx][Xx]|20[Xx][Xx])\\s*(\\-|\\/|:)\\s*(1[0-9][Xx][Xx]|20[Xx][Xx])\\s*$\"\n",
    "patterns[\"rangeYear2Digit\"]=\"^\\s*(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*(\\-|\\/|:)\\s*([0-9][0-9])\\s*$\"\n",
    "patterns[\"rangeDateVariant\"]=\"^\\s*([1-9]|0[1-9]|[12][0-9]|3[01])\\D([1-9]|0[1-9]|1[012])\\D(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*(\\-|\\/|:)\\s*([1-9]|0[1-9]|[12][0-9]|3[01])\\D([1-9]|0[1-9]|1[012])\\D(1[0-9][0-9][0-9]|20[0-9][0-9])\\s*$\"\n",
    "\n",
    "patterns[\"email\"]=\"^[_]*([a-z0-9]+(\\.|_*)?)+@([a-z][a-z0-9-]+(\\.|-*\\.))+[a-z]{2,6}$\"\n",
    "patterns[\"domain\"]=\"^([a-z][a-z0-9-]+(\\.|-*\\.))+[a-z]{2,6}$\"\n",
    "patterns[\"url\"]=\"^https?\\:\\/\\/[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,3}\\/?$\"\n",
    "patterns[\"ipv4\"]=\"^(?:\\d{1,3}\\.){3}\\d{1,3}$\"\n",
    "patterns[\"rgbHex\"]=\"^#([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$\"\n",
    "patterns[\"generalHex\"]=\"^#[a-fA-F0-9]*$\"\n",
    " \n",
    "patterns[\"isbnPrefix\"]=\"^ISBN(-1(?:(0)|3))?:?\\x20(\\s)*[0-9]+[- ][0-9]+[- ][0-9]+[- ][0-9]*[- ]*[xX0-9]$\"\n",
    "patterns[\"isbn\"]=\"^[0-9]+[- ][0-9]+[- ][0-9]+[- ][0-9]*[- ]*[xX0-9]$\"\n",
    "patterns[\"NaN\"]=\"^[Nn][Aa][Nn]$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to find out if all date columns match the patterns specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rowCount=0\n",
    "histogram=dict()\n",
    "for row in df.iterrows():\n",
    "    rowCount=rowCount+1\n",
    "    readDate=str(row[1][\"date\"])\n",
    "    matchedOnce=False\n",
    "    for key in patterns:\n",
    "        p=re.compile(patterns[key])\n",
    "        m = p.search(readDate)\n",
    "        if m:\n",
    "            if not key in histogram:\n",
    "                histogram[key]=0\n",
    "            histogram[key]=histogram[key]+1\n",
    "            matchedOnce=True\n",
    "        else:\n",
    "            pass\n",
    "    if not matchedOnce:\n",
    "        print(\"No matches at all: \"+row[1][\"PPN\"]+\"\\t for: \"+str(readDate))\n",
    "print(\"Row count: \"+str(rowCount))\n",
    "print(histogram)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*sorted(histogram.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the matching yields a \"sediment\" of extreme cases that need special treatment, e.g., \"1756 $ [ca. 1756]\". By iterating this process, we could easily address all patterns present in the dataset.\n",
    "\n",
    "### Inspecting Spatial Locations\n",
    "\n",
    "Because the dataset contains a lot of historical works, the _spatial_ columns deserves further attention. To get a feeling for the data, we will inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coverage.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the spatial column does not only contain cities.  Sometimes, more than one city or a even a country is contained. Another variant uses square brackets. Additionally, specific bibliographic terminology indicate that no place of publication ([s.l.]=sine loco=without place or o.O. as its German equivalent) could be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data Cleansing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    # matches alphanumeric character and the underscore at the beginning of the string\n",
    "    #Unicode flag is needed because of Asian character sets otherwise such signs would be considered as non-alphanumeric\n",
    "    regEx_AlphaNum=re.compile(\"^\\w\",re.UNICODE)\n",
    "    # checks for surrounding []; will match almost everything but Asian characters\n",
    "    regEx_BracketText=re.compile(\"^\\[[\\w\\?\\.,\\sßÄäÖöÜü]*\\]\",re.UNICODE)\n",
    "    # checks for typical spellings of the \"sine loco\" abbreviation \"s. l.\"\n",
    "    regEx_SineLoco=re.compile(\"[sSoO]\\s?\\.\\s?[lLoO]\\s?\\.?\\s?\",re.UNICODE)\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cleanAncientYearStrict(self,readData):\n",
    "        if type(readData)==float:\n",
    "            if not readData:\n",
    "                return 0\n",
    "            else:\n",
    "                return readData\n",
    "        else:\n",
    "            p=re.compile(patterns[\"ancientYear\"])\n",
    "            m = p.search(str(readData))\n",
    "            if m:\n",
    "                firstAppearance=m.group()\n",
    "                return firstAppearance\n",
    "            else:\n",
    "                return np.nan\n",
    "            \n",
    "    def cleanSpatialText(self,readData):\n",
    "        returnedString=\"\"\n",
    "        # just in case we did not get a string, we use brute force and return NaN\n",
    "        if type(readData)==float:\n",
    "            return \"\"\n",
    "        else:\n",
    "            #readData=str(readData)\n",
    "            m = self.regEx_AlphaNum.search(readData)\n",
    "            # if the string does start with a bracket...\n",
    "            if not m:\n",
    "                #print \"No matches at all: \"+row[1][\"PPN\"]+\"\\t for: \"+str(readData)\n",
    "                m2 = self.regEx_BracketText.search(readData)\n",
    "                if m2:\n",
    "                    matchedGroup=m2.group()\n",
    "                    #print \"\\tMatch: \"+matchedGroup\n",
    "                    m3=self.regEx_SineLoco.search(matchedGroup)\n",
    "                    if m3:\n",
    "                        #print \"\\tMatched Sine Loco: \"+str(m3.group())\n",
    "                        return \"\"\n",
    "                    else:\n",
    "                        matchedGroup=matchedGroup.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                        #print \"\\tFinal string: \"+matchedGroup\n",
    "                        returnedString=matchedGroup\n",
    "            # otherwise, it may still be a \"sine loco\"\n",
    "            else:\n",
    "                m3=self.regEx_SineLoco.search(readData)\n",
    "                if m3:\n",
    "                    #print \"\\tMatched Sine Loco: \"+str(m3.group())\n",
    "                    return \"\"\n",
    "                else:\n",
    "                    # in any case, there might be brackets left\n",
    "                    returnedString=readData.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        \n",
    "        # remove variants of \"u.a.\"            \n",
    "        regex = re.compile(\"[uU]\\.\\s?[aA]\\.\\s?\",re.UNICODE)\n",
    "        returnedString=regex.sub(\"\",returnedString)\n",
    "        return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dc=DataCleaner()\n",
    "\n",
    "#for row in df.iterrows():\n",
    "#    print dc.cleanSpatialText(str(row[1][\"spatial\"]))\n",
    "    \n",
    "df['spatialClean'] = df.coverage.apply(dc.cleanSpatialText)\n",
    "df['dateClean'] = df.date.apply(dc.cleanAncientYearStrict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#py27 df.sort_values(by=\"date\")\n",
    "\n",
    "# http://stackoverflow.com/questions/40032341/pandas-sort-dataframe-by-column-with-strings-and-integers\n",
    "df.groupby(df.date.apply(type) != str).apply(lambda g: g.sort('date')).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Clustering for Further Data Cleansing\n",
    "### Example of The Things We Are Up To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = u'Berlin Balin Cölln Köln'.split()\n",
    "print(words)\n",
    "\n",
    "print(\"Number of words: %i\" % len(words))\n",
    "for i,val in enumerate(words):\n",
    "    print(str(i)+\":\\t \"+str(val.encode('utf-8')))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are interested in finding out whether the Jaro distance is really a distance. As you might remember from your studies, a (normalized) distance yields 1.0 a dissimilar object. Hence, the distance between \"Berlin\" and \"Berlin\" should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaro_distance(u'Berlin',u'Berlin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it is not a distance. Therefore, we have to subtract the future results of __jaro_distance()__ from 1.0 to get a real distance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.triu_indices.html\n",
    "# 2nd parameter:\n",
    "# Diagonal above which to zero elements. k = 0 (the default) is the main diagonal, k < 0 is below it and k > 0 is above.\"\"\"\n",
    "# r= Return the indices for the upper-triangle of an (n, m) array. da m nicht angegeben ist, wird n=m angenommen\n",
    "# m is not passed, hence m=n\n",
    "\n",
    "# sagen, dass die matrix square ist!\n",
    "r=np.triu_indices(n=len(words), k=1)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what does this mean?\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "a_{0,0} & \\underline{a_{0,1}} & \\underline{a_{0,2}} & \\underline{a_{0,3}} \\\\\n",
    "\\cdot & a_{1,1} & \\underline{a_{1,2}} & \\underline{a_{1,3}} \\\\\n",
    "\\cdot & \\cdot & a_{2,2} & \\underline{a_{2,3}} \\\\\n",
    "\\cdot & \\cdot & \\cdot & a_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "how to interpret: $a_{0,1}$, i.e., the difference between \"Berlin\" and \"Balin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_demo(coord):\n",
    "    print(coord)\n",
    "    i, j = coord\n",
    "    # 1- wg. Distanz (see above)\n",
    "    return 1-jaro_distance(words[i], words[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.set_printoptions.html\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# axis (3rd parameter): 0= along y axis, 1= along x axis\n",
    "r2=np.apply_along_axis(d_demo, 0, r)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what does this mean for our matrix?\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "a_{0,0} & \\underline{0.1778} & \\underline{0.4222} & \\underline{0.3889} \\\\\n",
    "\\cdot & a_{1,1} & \\underline{0.4} & \\underline{0.3667} \\\\\n",
    "\\cdot & \\cdot & a_{2,2} & \\underline{0.2167} \\\\\n",
    "\\cdot & \\cdot & \\cdot & a_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "how to interpret: $a_{0,1}$, i.e., the difference between \"Berlin\" and \"Balin\" is 0.17777778.\n",
    "\n",
    "why not the elements on the diagonal? because...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Real Data\n",
    "\n",
    "first, we have to define the distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(coord):\n",
    "    #print(coord)\n",
    "    i, j = coord\n",
    "    #py27 return 1-jaro_distance(unicode(str(words[i]), 'utf-8'), unicode(str(words[j]), 'utf-8'))\n",
    "    return 1-jaro_distance(words[i],words[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df#pd27 .sort_values(by=\"date\")#.head(100)\n",
    "\n",
    "uniqueSpatials=df3[\"spatialClean\"].unique()\n",
    "words=None\n",
    "if demoClustering:\n",
    "    words=uniqueSpatials[:100] # only consider the first 100 elements for performance reasons\n",
    "else: # during normal operation, we want to consider all unique spatial names...\n",
    "    words=uniqueSpatials\n",
    "r=np.triu_indices(len(words), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next step will take some time, hence we limited the number of spatial labels before (3-4 min.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printLog(\"Started calculation of distance matrix for %i words...\"%len(words))\n",
    "# _ is the last evaluated value in an interactive shell\n",
    "# axis (3rd parameter): 0= along y axis, 1= along x axis\n",
    "r2=np.apply_along_axis(d, 0, r)\n",
    "printLog(\"Finished calculations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agglomeratives Clustering (hier nearest points, bottum-up) im gegensatz zu divisiven (top-down), das Beispiel hier nutzt den nearest point algorithm (Formel siehe https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z=scipycluster.linkage(r2)\n",
    "\n",
    "if not demoClustering:\n",
    "    pickle.dump( Z, open( \"cluster_hierarchy_linkage_result_without_name_clustering.pickle\", \"wb\" ) )\n",
    "\n",
    "#Z\n",
    "# scientific notation erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i müsste Zeile sein\n",
    "\n",
    "A 4 by (n-1) matrix Z is returned. At the i-th iteration, clusters with indices Z[i, 0] and Z[i, 1] are combined to form cluster n + i. A cluster with an index less than n corresponds to one of the n original observations. The distance between clusters Z[i, 0] and Z[i, 1] is given by Z[i, 2]. The fourth value Z[i, 3] represents the number of original observations in the newly formed cluster.\n",
    "\n",
    "mehr infos: https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z=pickle.load( open( \"cluster_hierarchy_linkage_result.pickle\", \"rb\" ) )\n",
    "if demoClustering:\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('sample index')\n",
    "    plt.ylabel('distance')\n",
    "    scipycluster.dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=16.,  # font size for the x axis labels\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* fcluster macht aus der clusterhierarchie wiederum \"flat clusters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=scipycluster.fcluster(Z, t=0.1,criterion=\"distance\")\n",
    "# 2. parameter ist abhängig von der clustering strategie, -> cophenetic distance\n",
    "# see: http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "# An array of length n. T[i] is the flat cluster number to which original observation i belongs.\n",
    "\n",
    "\n",
    "# https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cophenetic.html\n",
    "# https://en.wikipedia.org/wiki/Cophenetic\n",
    "# wir erhalten für jedes unser elemente eine cluster ID\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "service functions that help to establish a linkage between the cluster IDs and human readable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordIndex(word):\n",
    "    return np.where(words==word)[0]\n",
    "\n",
    "def getClusterID(data):\n",
    "#for row in df3.iterrows():\n",
    "    #data=row[1][\"spatialClean\"]\n",
    "    #wordIndex=np.where(words==data)[0]\n",
    "    #if data == u\"奈良\".encode('utf-8'):\n",
    "    #    print \"China!\"\n",
    "    #    wordIndex=getWordIndex(data)\n",
    "    #    print wordIndex\n",
    "    #    print clusters[wordIndex][0]\n",
    "    wordIndex=getWordIndex(data)\n",
    "    if wordIndex:\n",
    "        return clusters[wordIndex][0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Clustered Results Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "Z_huge=pickle.load( open( \"cluster_hierarchy_linkage_result_without_name_clustering.pickle\", \"rb\" ) )\n",
    "uniqueSpatials=df3[\"spatialClean\"].unique()\n",
    "words=uniqueSpatials\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=scipycluster.fcluster(Z_huge, t=0.07,criterion=\"distance\")\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['spatialCluster'] = df3[\"spatialClean\"].apply(getClusterID)\n",
    "grp=df3.groupby(\"spatialCluster\")\n",
    "#print grp.groups.keys()\n",
    "print(\"Number of clusters: %i\" % len(grp.groups.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stichproben ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Berlin\")][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shortcut, because we are only interested in the unique names within a cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Frankfurt/Oder\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semantisch sind die cluster natuerlich nicht korrekt...\n",
    "\n",
    "a good time for inspecting all of our clusters' contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in grp.groups.keys():\n",
    "    if key:\n",
    "        print(key)\n",
    "        print(grp.get_group(key)[\"spatialClean\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* synonym: ['Francofurti Ad Viadrum'] as a Latin translation of Frankfurt/Oder would be long to the cluster with ['Frankfurt/Main' 'Frankfurt/Oder' 'Frankfurt, Main' 'Frankfurt, O']\n",
    "* however, Frankfurt/Main and Frankfurt/Oder are to differt cities\n",
    "* ['Francofurti'] is in a 1-element cluster\n",
    "* ['C\\xc3\\xb6lln an der Spree'] is a synonym for Berlin\n",
    "* duplicate entries: ['Hallae Magdeburgicae  Hallae Magdeburgicae'] 1-gram und 2-gram vergleichen!\n",
    "* auf Enthaltensein von Berlin prüfen\n",
    "* St. Sankt Saint Bad als Präfix behandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup of the dataframe as we are going to remove data from the spatialClean column\n",
    "pickleCompress('./picklez/df3_unclean_spatials.picklez',df3)\n",
    "# if you want to continue from here, uncomment the following line\n",
    "#df3=pickleDecompress('./picklez/df3_unclean_spatials.picklez')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Cleaning\n",
    "\n",
    "on some occasions, we have to deal with spatialClean entries that contain multiple cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPN836134109 contains multiple cities\n",
    "df3[df3.PPN==\"PPN836134109\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle compound city names (only today's German cities to give an example)\n",
    "\n",
    "* to avoid superfluous comparisons we split the city names by their leading character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityNameFile_DE = open('citynames.txt', 'r')\n",
    "cityNGramsDE=dict()\n",
    "lastLetter=\"\"\n",
    "for line in cityNameFile_DE:\n",
    "    if not line.startswith(\"//\"):\n",
    "        r=line.replace(\"\\n\",\"\").strip()\n",
    "        if \" \" in r:\n",
    "            #print r\n",
    "            if not lastLetter==r[0].lower():\n",
    "                lastLetter=r[0].lower()\n",
    "                cityNGramsDE[lastLetter]=[]\n",
    "            cityNGramsDE[lastLetter].append(r)\n",
    "            #py27 cityNGramsDE[lastLetter].append(r.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in case of multiple cities, we will only consider the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various test cases of city names\n",
    "#s1=\"Frankfurt, O\"\n",
    "#s2='Hallae Magdeburgicae  Hallae Magdeburgicae'\n",
    "#s3=\"Leipzig  Paris  Petersburg  London\"\n",
    "#s4='Franckfurt  N\\xc3\\xbcrnberg  Leipzig'\n",
    "#s5='Freiburg i.Br. ' # not matched correctly but okay\n",
    "#s6='Frankfurt/Main' \n",
    "#s7='Frankfurt, Main'\n",
    "\n",
    "#s8='Bad Nauheim'\n",
    "#s9='Rottach-Egern am Tegernsee'\n",
    "#s10='Egern a. Tegernsee'\n",
    "#s11=\"Plancy-L'Abbaye\"\n",
    "#s12='Bad Nauheim Sankt'\n",
    "#s13=\"Saint Tropez\"\n",
    "#s14=\"Sankt Augustin\"\n",
    "#s15=\"Sankt-Augustin\"\n",
    "#s16=\"St.-Whatever\"\n",
    "#s17=\"St. Whatever\"\n",
    "\n",
    "def pickFirstCity(testString):\n",
    "# checks if the testString contains multiple cities separated by whitespaces and returns the first city respecting city name prefixes such as Saint, St. etc.\n",
    "    #py27 testString=unicode(testString,\"utf-8\")\n",
    "    #print type(testString)\n",
    "    #testString=testString.decode('unicode-escape')\n",
    "    \n",
    "    # matches for whitespaces that are NOT preceded by the following signs: \", ; : \\ / \" denoted in the regex by (?<!...)\n",
    "    regex = re.compile(\"(?<![,;:\\\\\\/])\\s*\",re.UNICODE)\n",
    "\n",
    "    # matches various city prefix such as Saint etc.\n",
    "    spatialPrefixRegExes=[]\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[Bb][Aa][Dd]\\s*\",re.UNICODE))\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[Ss][Aa][Nn][Kk][Tt][\\s-]*\",re.UNICODE))\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[Ss][Aa][Ii][Nn][Tt][\\s-]*\",re.UNICODE))\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[S][t]\\.[\\s-]*\",re.UNICODE))\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[Dd][Ee][Nn]\\s*\",re.UNICODE))\n",
    "    spatialPrefixRegExes.append(re.compile(\"^[Ne][Ee][Ww]\\s*\",re.UNICODE))\n",
    "\n",
    "    #print \"Tested string: >%s<\" % testString\n",
    "    \n",
    "    if allowExactStringMatching:\n",
    "        # checks whether the spatial is known to consist of more than one word\n",
    "        # for the sake of efficiency we will only check for spatial names with the same leading character (see above)\n",
    "        lowTestString=testString.lower()\n",
    "        if len(lowTestString)>=1:\n",
    "            currentLetter=lowTestString[0]\n",
    "            if currentLetter in cityNGramsDE:\n",
    "                for cityName in cityNGramsDE[currentLetter]:\n",
    "                    foundIndex=lowTestString.find(cityName.lower())\n",
    "                    #regexCity = re.compile(\"^\\b\"+cityName.lower()+\"\\b\",re.UNICODE)\n",
    "                    r=re.match(\"\\\\b\"+cityName.lower()+\"\\\\b\",lowTestString)\n",
    "                    #if foundIndex==0:\n",
    "                    if r is not None:\n",
    "                        #print \"Compound city: \"+cityName+\" @ \"+str(foundIndex)\n",
    "                        return cityName\n",
    "    \n",
    "    # general purpose splitting\n",
    "    foundSpatialPrefix=False\n",
    "    for i,r in enumerate(spatialPrefixRegExes):\n",
    "        m = r.search(testString)\n",
    "        if m:\n",
    "            #print \"Prefix %i\" %i\n",
    "            foundSpatialPrefix=True\n",
    "    #print type(testString)\n",
    "    m = regex.split(testString)\n",
    "    \n",
    "    if foundSpatialPrefix:\n",
    "        if len(m)>1:\n",
    "            return m[0]+\" \"+m[1]\n",
    "    else:\n",
    "        return m[0]\n",
    "\n",
    "print(pickFirstCity('Bad Königshofen im Grabfeld'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bevor man die erste stadt auswählt muss man, sich die einträge mit mehr als einer stadt speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "maxItems=df3.shape[0]\n",
    "\n",
    "multipleCitiesPPN=dict()\n",
    "\n",
    "printLog(\"Processing multiple spatial names entries...\")\n",
    "for row in df3.iterrows():\n",
    "    ppn=str(row[1][\"PPN\"])  \n",
    "    spatialC=str(row[1][\"spatialClean\"])\n",
    "    multipleCitiesPPN[ppn]=[]\n",
    "    \n",
    "    counter=counter+1\n",
    "    if counter%10000==0:\n",
    "        printLog(\"\\tProcessed %i items of %i\"%(counter,maxItems))\n",
    "                \n",
    "    if spatialC:\n",
    "        origSpatialClean=\"<\"+spatialC+\"> (\"+ppn+\")\"\n",
    "        f=pickFirstCity(spatialC)\n",
    "        loopCount=1\n",
    "        while f:\n",
    "            #print f\n",
    "            multipleCitiesPPN[ppn].append(f)\n",
    "            #py27 spatialC=spatialC.decode(\"utf-8\")\n",
    "            spatialC=re.sub('\\s+',' ',spatialC.replace(f,u\"\")).strip()\n",
    "            #py27 spatialC=spatialC.encode(\"utf-8\")\n",
    "            if spatialC:\n",
    "                #print \"<\"+f.encode(\"utf-8\")+\"> | <\"+spatialC+\">\"\n",
    "                #f=spatialC\n",
    "                f=pickFirstCity(spatialC)\n",
    "            else:\n",
    "                #print origSpatialClean\n",
    "                break\n",
    "            loopCount=loopCount+1\n",
    "            if loopCount>15:\n",
    "                printLog(\"Max. loop count reached @ \"+origSpatialClean+ \" with last f of: <\"+f+\">\")\n",
    "                break\n",
    "        #if f:\n",
    "            #spatialC=spatialC.decode(\"utf-8\")\n",
    "            #spatialC=spatialC.replace(f,u\"\").strip()\n",
    "            #spatialC=spatialC.encode(\"utf-8\")\n",
    "            #if spatialC:\n",
    "                #print origSpatialClean\n",
    "                #print \"<\"+f.encode(\"utf-8\")+\"> | <\"+spatialC+\">\"\n",
    "                #counter=counter+1\n",
    "                #if counter==10:\n",
    "                    #break\n",
    "    #print ppn\n",
    "    #break\n",
    "printLog(\"Done.\")\n",
    "pickleCompress(\"./picklez/multipleSpatialNamesPerPPN.picklez\",multipleCitiesPPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the PPNs with multiple spatial names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ppn in multipleCitiesPPN:\n",
    "    array=multipleCitiesPPN[ppn]\n",
    "    if len(array)>1:\n",
    "        #py27 print(ppn+\":\\t\"+\"; \".join(array).encode(\"utf-8\"))\n",
    "        print(ppn+\":\\t\"+\"; \".join(array))\n",
    "        #for e in array:\n",
    "            #print e.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* interessante PPN: PPN780104447; PPN771083963; PPN792355296; PPN735012342\n",
    "* Umgang mit ? klären\n",
    "* Einträge des Arrays übeprüfen: Propositionen (auch Mehrwerte-Komposita wie \"an der\"), Nicht-Ortsnamen (z.B. Poststempel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* finally, we can extract the first cities\n",
    "\n",
    "* next cell might take ca. 3 min. with list-based exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printLog(\"Detecting and picking first cities...\")\n",
    "uniqueSpatials=df3[\"spatialClean\"].unique()\n",
    "beforeClusterClean=len(uniqueSpatials)\n",
    "df3[\"spatialClean\"]=df3[\"spatialClean\"].apply(pickFirstCity)\n",
    "printLog(\"Done.\")\n",
    "\n",
    "if allowExactStringMatching:\n",
    "    pickleCompress(\"./picklez/df3_exactmatch_cities.picklez\",df3)\n",
    "else:\n",
    "    df3=pickleDecompress(\"./picklez/df3_exactmatch_cities.picklez\")\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.coverage.notnull()].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gate 4\n",
    "\n",
    "check our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueSpatials=df3[\"spatialClean\"].unique()\n",
    "afterClusterClean=len(uniqueSpatials)\n",
    "words=uniqueSpatials\n",
    "#words\n",
    "print(\"Before cluster cleaning: %i\" % beforeClusterClean)\n",
    "print(\"After cluster cleaning: %i\" % afterClusterClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2(coord):\n",
    "    #print \"Altered d()\"\n",
    "    #print coord\n",
    "    i, j = coord\n",
    "    #print(str(type(words[i]))+\" : \"+str(type(words[j])))\n",
    "    #py27 if not type(words[i])==unicode:\n",
    "        #print \"bumm \"+ str(words[i])+\" :\"+str(i)\n",
    "    if not words[i]:\n",
    "        return 1.0\n",
    "    #py27 if not type(words[j])==unicode:\n",
    "        #print \"bamm \" + str(words[j])+\" :\"+str(j)\n",
    "    if not words[j]:\n",
    "        return 1.0\n",
    "    dist=1 - jaro_distance(words[i],words[j]) # because jaro_distance is actually returning a similarity\n",
    "    #print \"%s vs. %s -> %f\" %(words[i],words[j],dist)\n",
    "    #return 1 - jaro_distance(unicode(str(words[i]), 'utf-8'), unicode(str(words[j]), 'utf-8'))\n",
    "    return dist\n",
    "\n",
    "def d3(coord):\n",
    "    #print \"Altered d()\"\n",
    "    #print coord\n",
    "    i, j = coord\n",
    "    #print str(type(words[i]))+\" : \"+str(type(words[j]))\n",
    "    #py27 if not type(words[i])==unicode:\n",
    "        #print \"bumm \"+ str(words[i])+\" :\"+str(i)\n",
    "    if not words[i]:\n",
    "        return 1.0\n",
    "    #py27 if not type(words[j])==unicode:\n",
    "        #print \"bamm \" + str(words[j])+\" :\"+str(j)\n",
    "    if not words[j]:\n",
    "        return 1.0\n",
    "    dist=1 - jaro_winkler(words[i],words[j]) # because jaro_distance is actually returning a similarity\n",
    "    #print \"%s vs. %s -> %f\" %(words[i],words[j],dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vielleicht nötig, das ganze außerhalb des jupyter-notebooks auszuführen, wenn es time out probleme (siehe console) gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not demoClustering:\n",
    "    printLog(\"Calculating Jaro distances...\")\n",
    "    r=np.triu_indices(afterClusterClean, 1)\n",
    "    r2=np.apply_along_axis(d2, 0, r)\n",
    "    Z=scipycluster.linkage(r2)\n",
    "    pickle.dump( Z, open( \"cluster_hierarchy_linkage_result.pickle\", \"wb\" ) )\n",
    "    \n",
    "    # Jaro-Winkler\n",
    "    printLog(\"Calculating Jaro-Winkler distances...\")\n",
    "    r=np.triu_indices(afterClusterClean, 1)\n",
    "    r3=np.apply_along_axis(d3, 0, r)\n",
    "    Z3=scipycluster.linkage(r3)\n",
    "    pickle.dump( Z3, open( \"cluster_hierarchy_linkage_jw_result.pickle\", \"wb\" ) )\n",
    "    #pickle.dump( r2, open( \"r2.pickle\", \"wb\" ) )\n",
    "    printLog(\"Distance matrices created. Done.\")\n",
    "    # (END_HERE_FOR_LINKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the \"Final\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load stuff\n",
    "printLog(\"Loading final data...\")\n",
    "# Jaro\n",
    "#Z_huge=pickle.load( open( \"cluster_hierarchy_linkage_result.pickle_bak\", \"rb\" ) )\n",
    "# Jaro-Winkler\n",
    "Z_huge=pickle.load( open( \"cluster_hierarchy_linkage_jw_result.pickle\", \"rb\" ) )\n",
    "\n",
    "#r2=pickle.load( open( \"r2.pickle\", \"rb\" ) )\n",
    "printLog(\"Loading final data completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group records by their cluster and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueSpatials=df3[\"spatialClean\"].unique()\n",
    "words=uniqueSpatials\n",
    "print(len(words))\n",
    "# Jaro\n",
    "#clusters=scipycluster.fcluster(Z_huge, t=0.115,criterion=\"distance\") # je höher d, desto mehr kommt in 1 cluster\n",
    "# Jaro-Winkler\n",
    "clusters=scipycluster.fcluster(Z_huge, t=0.095,criterion=\"distance\") # je höher d, desto mehr kommt in 1 cluster\n",
    "\n",
    "print(clusters)\n",
    "df3['spatialCluster'] = df3[\"spatialClean\"].apply(getClusterID)\n",
    "grp=df3.groupby(\"spatialCluster\")\n",
    "\n",
    "print(\"Number of clusters: %i\" % len(grp.groups.keys()))\n",
    "\n",
    "# save the clusters' names (for simplicity, we just take the name of the cluster's first element)\n",
    "clusterNames=dict()\n",
    "maxKey=-1\n",
    "for key in grp.groups.keys():\n",
    "    if key:\n",
    "        if key>maxKey:\n",
    "            maxKey=key\n",
    "        if not key in clusterNames:\n",
    "            clusterNames[key]=grp.get_group(key)[\"spatialClean\"].unique()[0]\n",
    "            \n",
    "        print(str(key)+\" <\"+clusterNames[key]+\"> contains:\\n\\t\"+str(grp.get_group(key)[\"spatialClean\"].unique()))\n",
    "print(\"Max key value: %i\"%maxKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Moskau\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Bamberg\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Berlin\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Frankfurt\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Hamburg\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(clusters[getWordIndex(\"Uri\")][0])[\"spatialClean\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing out the spatial cluster names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterName(clusterID):\n",
    "    if clusterID:\n",
    "        if clusterID in clusterNames:\n",
    "            r=clusterNames[clusterID]\n",
    "            if r:\n",
    "                return r.title()\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df3['spatialClusterName'] = df3[\"spatialCluster\"].apply(getClusterName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual corrections of the spatial cluster names (über Karte gesehen)\n",
    "* Carlsruhe\n",
    "* Crefeld\n",
    "* Weymar\n",
    "* Kjøbenhavn\n",
    "* Leipzig, Dresden\n",
    "* St. Peterburg\n",
    "* Coblentz\n",
    "* Halæ\n",
    "\n",
    "wäre besser, das vor dem Clustern zu tun\n",
    "\n",
    "it would be cumbersome to correct everything manually without further assistance as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.ix[df3[\"spatialClusterName\"]==\"Carlsruhe\", \"spatialClusterName\"] = \"Karlsruhe\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==\"Crefeld\", \"spatialClusterName\"] = \"Krefeld\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==\"Weymar\", \"spatialClusterName\"] = \"Weimar\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==u\"Kjøbenhavn\", \"spatialClusterName\"] = u\"København\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==\"Leipzig, Dresden\", \"spatialClusterName\"] = \"Leipzig\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==\"St. Peterburg\", \"spatialClusterName\"] = \"Sankt Petersburg\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==\"Coblentz\", \"spatialClusterName\"] = \"Koblenz\"\n",
    "df3.ix[df3[\"spatialClusterName\"]==u\"Halæ\", \"spatialClusterName\"] = u\"Halle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gate 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if there was no cluster name, just take the cleaned spatial name\n",
    "df3.spatialClusterName.fillna(df3.spatialClean, inplace=True)\n",
    "uniqueValues(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in the next step, we will extract all unique spatial cluster names for manual correction\n",
    "* an Excel sheet is created which also contains statistical data about the name in order to reveal outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValues={'SpatialClusterName':[],'Length':[],'NoOfElements':[],'Entries':[]}\n",
    "\n",
    "sortedUniqueDF3=df3.sort_values(by=\"spatialClusterName\")[\"spatialClusterName\"].unique()\n",
    "\n",
    "for uniqueName in sortedUniqueDF3:\n",
    "    if uniqueName:\n",
    "        length=str(len(uniqueName))\n",
    "        entries=df3[df3[\"spatialClusterName\"]==uniqueName][\"spatialClean\"].unique()\n",
    "        noOfElements=str(len(entries))\n",
    "        entriesString=\",\".join(entries)\n",
    "        dfValues['SpatialClusterName'].append(uniqueName)\n",
    "        dfValues['Length'].append(length)\n",
    "        dfValues['NoOfElements'].append(noOfElements)\n",
    "        dfValues['Entries'].append(entriesString)\n",
    "\n",
    "spDF=pd.DataFrame(dfValues)\n",
    "spDF = spDF[['SpatialClusterName', 'Length','NoOfElements','Entries']]\n",
    "spDF['NoOfElements'] = pd.to_numeric(spDF['NoOfElements'])\n",
    "spDF['Length'] = pd.to_numeric(spDF['Length'])\n",
    "spDF['NewName']=\"\"\n",
    "\n",
    "stdLength=spDF.std()[\"Length\"]\n",
    "avgLength=spDF.mean()[\"Length\"]\n",
    "lowerLength=avgLength-stdLength\n",
    "upperLength=avgLength+stdLength\n",
    "\n",
    "stdNoOfElements=spDF.std()['NoOfElements']\n",
    "avgNoOfElements=spDF.mean()['NoOfElements']\n",
    "lowerNoOfElements=avgNoOfElements-stdNoOfElements\n",
    "upperNoOfElements=avgNoOfElements+stdNoOfElements\n",
    "\n",
    "def atypicalLength(x):\n",
    "    if x < lowerLength or x > upperLength:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def atypicalNoOfElements(x):\n",
    "    if x < lowerNoOfElements or x > upperNoOfElements:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "spDF['AtypicalLength']=spDF['Length'].apply(atypicalLength)\n",
    "spDF['AtypicalNoOfElements']=spDF['NoOfElements'].apply(atypicalNoOfElements)\n",
    "\n",
    "spDF.to_excel(\"./spatialnames.xlsx\",index=False)\n",
    "spDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpNoOfElements=spDF.groupby(\"NoOfElements\")\n",
    "grpNoOfElements.count().plot()\n",
    "if saveFiguresAsPDF:\n",
    "    plt.savefig('./figures/noelements_cluster.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpLength=spDF.groupby(\"Length\")\n",
    "grpLength.count().plot()\n",
    "if saveFiguresAsPDF:\n",
    "    plt.legend(\"\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    plt.xlabel(\"Length of Cluster Name\")\n",
    "    plt.savefig('./figures/length_spatialnames.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Poststempel\", \"Erscheinungsort\", nicht entzifferbare Abkürzungen lassen sich so auffinden oder: St.Moritz Hotel, bzw. Kombinationen aus Verlagen und Orten; Signaturen/Referenzen: \"Br117\"; \"VIII, 35\"; (Budae,Budapest,Budapesten) wird als ein Cluster erkannt\n",
    "\n",
    "* Excel weißt statistische Besonderheiten aus und ist alphabetisch anhand der Cluster sortiert, so kommt man in gut XX Minuten mit der Korrektur durch\n",
    "\n",
    "* nicht alle Korrekturen sind manuell notwendig, da Ortsnamen auch Mehrsprachig in OSM oder GND(???) liegen\n",
    "* Character Encoding -Probleme sind dort auch gut zu erkennen (Piešťany)\n",
    "\n",
    "* consonant shift or sound changes\n",
    "\n",
    "* durch die Aufbereitung in ca. 45 Minuten leistbar (inkl. Recherchen in Historischen Ortsnamen, OSM und Google)\n",
    "\n",
    "Manual correction should be done in _spatialnamesCorrections.xlsx_. For illustration purposes, some correction have been made. Dots indicate rows for which the spatial name cluster will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spDF_manuallyCleaned=pd.read_excel(\"spatialnamesCorrections.xlsx\")\n",
    "spDF_manuallyCleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.spatialClusterName==\"Aigen/Salzburg\"].head() # returns PPN771101503 and/or PPN771101589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.spatialClusterName==\"1\"].head() # returns PPN662043146 and PPN688520944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## korrekturen einarbeiten; sagen, welche spalten wie ausgefüllt werden müssen; vorteil excel: leute kennen das tool und können sich alles anpassen, wie sie s brauchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.ix[df3[\"spatialClusterName\"]==\"Carlsruhe\", \"spatialClusterName\"] = \"Karlsruhe\"\n",
    "printLog(\"Fixing spatial cluster names on the basis of manual corrections...\")\n",
    "counterRemoved=0\n",
    "counterAltered=0\n",
    "ignoredSpatialNamesLowerCase=[]\n",
    "\n",
    "for row in spDF_manuallyCleaned.iterrows():\n",
    "    #py27 newName=readDate=unicode(row[1][\"NewName\"])\n",
    "    #py27 oldName=unicode(row[1][\"SpatialClusterName\"])\n",
    "    newName=str(row[1][\"NewName\"])\n",
    "    oldName=row[1][\"SpatialClusterName\"]\n",
    "    \n",
    "    if newName==\".\":\n",
    "        #print(\"Removed: \"+row[1][\"SpatialClusterName\"])\n",
    "        df3.ix[df3[\"spatialClusterName\"]==oldName, \"spatialClusterName\"] = \"\"\n",
    "        ignoredSpatialNamesLowerCase.append(oldName.lower())\n",
    "        counterRemoved=counterRemoved+1\n",
    "    elif newName==\"nan\":\n",
    "        pass\n",
    "    else:\n",
    "        #print(\"Changed: \"+row[1][\"SpatialClusterName\"])\n",
    "        df3.ix[df3[\"spatialClusterName\"]==oldName, \"spatialClusterName\"] = newName\n",
    "        counterAltered=counterAltered+1\n",
    "        \n",
    "printLog(\"Done.\")\n",
    "printLog(\"Removed clusters: %i\"%counterRemoved)\n",
    "printLog(\"Altered clusters: %i\"%counterAltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gate 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueValues(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spatialCluster-ID muss auch noch angepasst werden und der spatialClusterName-Anzahl entsprechen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some debugging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.PPN==\"PPN771101503\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.PPN==\"PPN662043146\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickleCompress('./picklez/clean_dataframe.picklez',df3)\n",
    "pickleCompress('./picklez/ignoredSpatialNames.picklez',ignoredSpatialNamesLowerCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geodaten erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment only needed if you resume the notebook\n",
    "df4=pickleDecompress('./picklez/clean_dataframe.picklez')\n",
    "#ignoredSpatialNamesLowerCase=pickleDecompress('./picklez/ignoredSpatialNames.picklez')\n",
    "\n",
    "# otherwise just run this line:\n",
    "#df4=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not tested with Python 3\n",
    "if useGoogleMapsAPI:\n",
    "    # idee ist, die Algorithmen von Google zu nutzen, um mit mitunter fehlerhaften, mehrsprachigen Ortsbezeichner umzugehen\n",
    "    gmaps = googlemaps.Client(key=privateGoogleKey)\n",
    "    # if you re-run this cell you might want to uncomment the following line\n",
    "    latLng=dict()\n",
    "    printLog(\"Fetching geolocations from Google Maps...\")\n",
    "    for row in df4.iterrows():\n",
    "        if not row[1][\"spatialClusterName\"]:\n",
    "            # in some cases, i.e., when the cluster was corrected manually above, \n",
    "            # we will ignore the spatialClean replacement\n",
    "            loc=row[1][\"spatialClean\"]\n",
    "            if loc:\n",
    "                if not loc.lower() in ignoredSpatialNamesLowerCase:\n",
    "                    printLog(\"Taking %s instead.\"%loc)\n",
    "                else:\n",
    "                    loc=None\n",
    "        else:\n",
    "            loc=row[1][\"spatialClusterName\"]\n",
    "        if loc:\n",
    "            if loc not in latLng:\n",
    "                #printLog(loc)\n",
    "                try:\n",
    "                    retLL=gmaps.geocode(loc)\n",
    "                    if len(retLL)>0:\n",
    "                        latLng[loc]=retLL[0][u'geometry'][u'location']\n",
    "                    else:\n",
    "                        latLng[loc]=None\n",
    "                except googlemaps.exceptions.ApiError:\n",
    "                    printLog(\"Problem fetching: \"+loc)\n",
    "                    latLng[loc]=None\n",
    "    printLog(\"Done.\")\n",
    "\n",
    "    triedAgain=[]\n",
    "    printLog(\"Fetching geolocations from Google Maps a second time (aka the brute force failover solution)...\")\n",
    "    for row in df4.iterrows():\n",
    "        if not row[1][\"spatialClusterName\"]:\n",
    "            loc=row[1][\"spatialClean\"]\n",
    "        else:\n",
    "            loc=row[1][\"spatialClusterName\"]\n",
    "        if loc:\n",
    "            if latLng[loc]==None:\n",
    "                if loc not in triedAgain:\n",
    "                    #printLog(loc)\n",
    "                    triedAgain.append(loc)\n",
    "                    try:\n",
    "                        retLL=gmaps.geocode(loc)\n",
    "                        if len(retLL)>0:\n",
    "                            latLng[loc]=retLL[0][u'geometry'][u'location']\n",
    "                        else:\n",
    "                            latLng[loc]=None\n",
    "                    except googlemaps.exceptions.ApiError:\n",
    "                        printLog(\"Problem fetching: \"+loc)\n",
    "                        latLng[loc]=None\n",
    "    printLog(\"Done.\")\n",
    "    pickleCompress('./picklez/geo_coordinates.picklez',latLng)\n",
    "else:\n",
    "    printLog(\"Skipping Google Map API calls.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative with OpenStreetMap http://wiki.openstreetmap.org/wiki/Nominatim#Reverse_Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not useGoogleMapsAPI:\n",
    "    if getSpatialNamesFromOSM:\n",
    "        osmNominatimURL=\"http://nominatim.openstreetmap.org/search?format=json&namedetails=1&q=\"\n",
    "        # if you re-run this cell you might want to uncomment the following line\n",
    "        latLng=dict()\n",
    "        names=dict()\n",
    "    \n",
    "        maxItems=df4.shape[0]\n",
    "        counter=0\n",
    "    \n",
    "        printLog(\"Fetching geolocations from OpenStreetMaps...\")\n",
    "        for row in df4.iterrows():\n",
    "            counter=counter+1\n",
    "            if counter%5000==0:\n",
    "                printLog(\"\\tProcessed %i items of %i\"%(counter,maxItems))\n",
    "            if not row[1][\"spatialClusterName\"]:\n",
    "                # in some cases, i.e., when the cluster was corrected manually above, \n",
    "                # we will ignore the spatialClean replacement\n",
    "                loc=str(row[1][\"spatialClean\"])\n",
    "                if loc:\n",
    "                    if not loc.lower() in ignoredSpatialNamesLowerCase:\n",
    "                        printLog(\"No spatial cluster name for %s. Taking %s instead.\"%(row[1][\"PPN\"],loc))\n",
    "                    else:\n",
    "                        loc=None\n",
    "            else:\n",
    "                loc=str(row[1][\"spatialClusterName\"])\n",
    "            if loc:\n",
    "                if loc not in latLng:\n",
    "                #printLog(loc)\n",
    "                    locURL=osmNominatimURL+urllib.parse.quote(loc).replace(\" \",\"+\")\n",
    "                    try:\n",
    "                        data = json.load(urllib.request.urlopen(locURL))\n",
    "                        latLng[loc]=dict()\n",
    "                        if len(data)>0:\n",
    "                            #print data[0]\n",
    "                            latLng[loc][u'lat']=data[0][\"lat\"]\n",
    "                            latLng[loc][u'lng']=data[0][\"lon\"]\n",
    "                            #print loc\n",
    "                            names[loc]=dict()\n",
    "                            if len(data[0]['namedetails'])>0:\n",
    "                                for k,v in data[0]['namedetails'].items():\n",
    "                                    names[loc][k]=v\n",
    "                            else:\n",
    "                                names[loc][u'name']=loc\n",
    "                                #print \"No namedetails for \"+loc\n",
    "                            #print names[loc]\n",
    "                        else:\n",
    "                            latLng[loc]=None\n",
    "                    except IOError:\n",
    "                        printLog(\"\\tCould not open: \"+locURL)\n",
    "                        pickleCompress('./picklez/save_names.picklez',names)\n",
    "                        pickleCompress('./picklez/save_latLng.picklez',latLng)\n",
    "                    time.sleep(1) # see http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy\n",
    "        printLog(\"Number of coordinates: %i\"%len(latLng))\n",
    "        printLog(\"Done.\")\n",
    "    else:\n",
    "        printLog(\"Skipping OpenStreetMap API calls.\")\n",
    "else:\n",
    "    printLog(\"Skipping OpenStreetMap API calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for manual additions, you can directly pass an URL as the example below shows\n",
    "#manualLocation=\"Kehl\"\n",
    "#data = json.load(urllib.urlopen(\"http://nominatim.openstreetmap.org/search?format=json&namedetails=1&q=\"+manualLocation))\n",
    "#loc=manualLocation\n",
    "#if len(data)>0:\n",
    "#    latLng[loc][u'lat']=data[0][\"lat\"]\n",
    "#    latLng[loc][u'lng']=data[0][\"lon\"]\n",
    "#    names[loc]=dict()\n",
    "#    if len(data[0]['namedetails'])>0:\n",
    "#        for k,v in data[0]['namedetails'].items():\n",
    "#            names[loc][k]=v\n",
    "#    else:\n",
    "#        names[loc][u'name']=loc\n",
    "#else:\n",
    "#    latLng[loc]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save everything\n",
    "if not useGoogleMapsAPI:\n",
    "    if getSpatialNamesFromOSM:\n",
    "        pickleCompress('./picklez/osm_names.picklez',names)\n",
    "        pickleCompress('./picklez/osm_latLng.picklez',latLng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if useGoogleMapsAPI:\n",
    "    latLng=pickleDecompress('./picklez/geo_coordinates.picklez')\n",
    "else:\n",
    "    latLng=pickleDecompress('./picklez/osm_latLng.picklez')\n",
    "    osmNames=pickleDecompress('./picklez/osm_names.picklez')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "latitude und longitude aus den spatial names generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLat(spatialName):\n",
    "    if spatialName:\n",
    "        if spatialName in latLng:\n",
    "            if latLng[spatialName]:\n",
    "                return latLng[spatialName][u'lat']\n",
    "            else:\n",
    "                return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def getLng(spatialName):\n",
    "    if spatialName:\n",
    "        if spatialName in latLng:\n",
    "            if latLng[spatialName]:\n",
    "                return latLng[spatialName][u'lng']\n",
    "            else:\n",
    "                return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df4['latitude']=df4['spatialClusterName'].apply(getLat)\n",
    "df4['longitude']=df4['spatialClusterName'].apply(getLng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda-Ausdruck ansprechen (Church aus theoretischer Informatik...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overwrite every \"nulled\" column (i.e. NULL or NaN) with -1\n",
    "df4.ix[df4[\"dateClean\"].isnull(), \"dateClean\"] = -1\n",
    "df4.ix[df4[\"dateClean\"]=='nan', \"dateClean\"] = -1\n",
    "\n",
    "df4[\"century\"]=df4[\"dateClean\"].apply(lambda x: int(int(x)/100)) # uns interessiert nur das jahrhundert\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpCentury=df4.groupby(\"century\")\n",
    "grpCentury.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=grpCentury.count().plot()\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Number of unique title fields %i\"%len(df4.title.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title soll geclustert werden -> textCluster\n",
    "corpus=df4.title.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will take a while to compute depending on your computer and the size of the corpus. To give an example, the following performance could be observed on a MacBook Pro (13-inch, 2018, Four Thunderbolt 3 Ports, 2,7 GHz Intel Core i7, 16 GB RAM):\n",
    "```\n",
    "[2019-02-25 11:08:53.568526]\tClustering text with cluster target size 5000\n",
    "[2019-02-25 11:08:53.569143]\t\t Preparing tf*idf model\n",
    "[2019-02-25 11:08:56.634801]\t\t Number of feature names: 125000\n",
    "[2019-02-25 11:08:56.635519]\t\t K-Means clustering\n",
    "[2019-02-25 11:17:49.427231]\tDone.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python 3 you cannot save files over 4 GB, hence we have to run the clustering everytime  :-/\n",
    "true_k=5000\n",
    "\n",
    "if sys.version_info.major>=3 or allowComputationallyExpensiveCalculations:\n",
    "    printLog(\"Clustering text with cluster target size %i\"%true_k)\n",
    "    printLog(\"\\t Preparing tf*idf model\")\n",
    "    # from an analysis we know that there are 167,715 different words in the corpus\n",
    "    # to speed up processing, we limit the vocabulary size to the ca. top-75% of the words\n",
    "    tfidfvectorizer = TfidfVectorizer(min_df=1,max_features=125000) #max_features is used to limit the vocabulary size \n",
    "    Xtfidf=tfidfvectorizer.fit_transform(corpus)\n",
    "    featNames=tfidfvectorizer.get_feature_names()\n",
    "    printLog(\"\\t Number of feature names: %i\"%len(featNames))\n",
    "\n",
    "    printLog(\"\\t K-Means clustering\")\n",
    "    km=MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    # fit the k-means algorithm on the data created above\n",
    "    km.fit(Xtfidf)\n",
    "    if sys.version_info.major<3:\n",
    "        pickleCompress('./picklez/textClustersK_'+str(true_k)+'.picklez',km)\n",
    "else:\n",
    "    km=pickleDecompress('./picklez/textClustersK_'+str(true_k)+'.picklez')\n",
    "    printLog(\"\\t Updating Dataframe\")\n",
    "# add the detected clusters as a new column to the original data frame\n",
    "df4['textCluster']=km.labels_\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allowComputationallyExpensiveCalculations:\n",
    "    # group the data by the cluster and describe it\n",
    "    df4.groupby('textCluster').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df4.sort_values(by=\"textCluster\")\n",
    "df4[['PPN','title','textCluster']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same with the creator column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creator soll geclustert werden -> creatorCluster\n",
    "corpus=df4.creator.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Number of unique creator fields %i\"%len(df4.creator.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[2019-02-25 11:18:40.288281]\tClustering text with cluster target size 20000\n",
    "[2019-02-25 11:18:40.289133]\t\t Preparing tf*idf model\n",
    "[2019-02-25 11:18:40.958502]\t\t Number of feature names: 26575\n",
    "[2019-02-25 11:18:40.959311]\t\t K-Means clustering\n",
    "[2019-02-25 11:26:49.960392]\tDone.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=20000\n",
    "\n",
    "if sys.version_info.major>=3 or allowComputationallyExpensiveCalculations:\n",
    "    printLog(\"Clustering text with cluster target size %i\"%true_k)\n",
    "    printLog(\"\\t Preparing tf*idf model\")\n",
    "    tfidfvectorizer = TfidfVectorizer(min_df=1) #max_features setzen?\n",
    "    Xtfidf=tfidfvectorizer.fit_transform(corpus)\n",
    "    featNames=tfidfvectorizer.get_feature_names()\n",
    "    printLog(\"\\t Number of feature names: %i\"%len(featNames))\n",
    "\n",
    "    printLog(\"\\t K-Means clustering\")\n",
    "    km=MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    # fit the k-means algorithm on the data created above\n",
    "    km.fit(Xtfidf)\n",
    "    if sys.version_info.major<3:\n",
    "        pickleCompress('./picklez/creatorClustersK_'+str(true_k)+'.picklez',km)\n",
    "else:\n",
    "    km=pickleDecompress('./picklez/creatorClustersK_'+str(true_k)+'.picklez')\n",
    "\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the detected clusters as a new column to the original data frame\n",
    "printLog(\"\\t Updating Dataframe\")\n",
    "df4['creatorCluster']=km.labels_\n",
    "\n",
    "df4=df4.sort_values(by=\"creatorCluster\")\n",
    "df4[['PPN','creator','creatorCluster']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageBasePath=\"/Users/david/src/__datasets/\"\n",
    "def getImageAvailability(ppn):\n",
    "    if ppn:\n",
    "        if os.path.exists(imageBasePath+\"firstpages/\"+ppn+\".jpg\"):\n",
    "            return \"firstpages/\"+ppn+\".jpg\"\n",
    "        else:\n",
    "            if os.path.exists(imageBasePath+\"titlepages/\"+ppn+\".jpg\"):\n",
    "                return \"titlepages/\"+ppn+\".jpg\"\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "printLog(\"Checking image availability...\")\n",
    "df4['titleImage']=df4['PPN'].apply(getImageAvailability)\n",
    "\n",
    "recordsWithTitleImage=df4[df4.titleImage.notnull()].shape[0]\n",
    "recordsWithoutTitleImage=df4[df4.titleImage.isnull()].shape[0]\n",
    "printLog(\"Records with title image: %i\\twithout title image (multi-volumes): %i\"%(recordsWithTitleImage,recordsWithoutTitleImage))\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickleCompress('./picklez/clean_dataframe_with_century.picklez',df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"cleanedData.csv\",sep=';',header=True, index=False, encoding='utf-8')\n",
    "df4.to_excel(\"cleanedData.xlsx\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analyses Shall Begin!\n",
    "safe to continue from here if you resume the notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have run this notebook from top to bottom, you don't have to reload the data\n",
    "df4=pickleDecompress('./picklez/clean_dataframe_with_century.picklez')\n",
    "grpCentury=df4.groupby(\"century\")\n",
    "#df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding things up\n",
    "\n",
    "dauert ca.40 min mit Zugriff innerhalb der Schleife mittels df4[df4.PPN==ppn].iloc[-1]['title'], deshalb look-up, so dauert alles komplett ca. 40 Sekunden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Creating PPN look-up table of %i unique PPNs.\"%len(df4.PPN.unique()))\n",
    "ppnLookup=dict()\n",
    "for row in df4.iterrows():\n",
    "    ppn=readDate=str(row[1][\"PPN\"])\n",
    "    if ppn not in ppnLookup:\n",
    "        ppnLookup[ppn]=dict()\n",
    "    # it is redundant but handy for later JSON exports to save the PPN as a value as well\n",
    "    ppnLookup[ppn][\"ppn\"]=ppn\n",
    "    ppnLookup[ppn][\"dateClean\"]=str(row[1][\"dateClean\"])\n",
    "    ppnLookup[ppn][\"title\"]=str(row[1][\"title\"])\n",
    "    ppnLookup[ppn][\"creator\"]=str(row[1][\"creator\"])\n",
    "    \n",
    "    if not row[1][\"spatialClusterName\"]:\n",
    "        if row[1][\"spatialClean\"]:\n",
    "            ppnLookup[ppn][\"spatialClusterName\"]=row[1][\"spatialClean\"]\n",
    "        else:\n",
    "            ppnLookup[ppn][\"spatialClusterName\"]=\"None\"\n",
    "    else:\n",
    "        ppnLookup[ppn][\"spatialClusterName\"]=row[1][\"spatialClusterName\"] \n",
    "    ppnLookup[ppn]['spatialClean']=row[1][\"spatialClean\"]\n",
    "    ppnLookup[ppn]['spatialRaw']=str(row[1][\"coverage\"])\n",
    "    ppnLookup[ppn]['mediatype']=str(row[1][\"type\"])\n",
    "    ppnLookup[ppn]['subject']=str(row[1][\"subject\"])\n",
    "    # daz\n",
    "    #ppnLookup[ppn]['source']=str(row[1][\"source\"])\n",
    "    ppnLookup[ppn]['publisher']=str(row[1][\"publisher\"])\n",
    "    #ppnLookup[ppn]['alternative']=str(row[1][\"alternative\"])\n",
    "    ppnLookup[ppn]['lat']=str(row[1][\"latitude\"])\n",
    "    ppnLookup[ppn]['lng']=str(row[1][\"longitude\"])\n",
    "    ppnLookup[ppn]['textCluster']=str(row[1][\"textCluster\"])\n",
    "    ppnLookup[ppn]['creatorCluster']=str(row[1][\"creatorCluster\"])\n",
    "    ppnLookup[ppn]['titleImage']=str(row[1][\"titleImage\"])\n",
    "    \n",
    "pickleCompress(\"./picklez/ppnLookup.picklez\",ppnLookup)\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize data per PPN; one large file would be more than 80 MB and thus too much for a transmission to a browser\n",
    "if serializePPNLookup2JSON:\n",
    "    printLog(\"Serializing PPN lookup table to JSON in directory:\\n\\t\"+jsonWebDir)\n",
    "    for k in ppnLookup:\n",
    "        dump=json.dumps(ppnLookup[k])\n",
    "\n",
    "        jsonFile = open(jsonWebDir+k+\".json\", \"w\")\n",
    "        jsonFile.write(dump)\n",
    "        jsonFile.close()\n",
    "    printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Creation and Its Place of Origin - Graphing the Library World\n",
    "build a network graph: do publishers and their publication locations resemble superheroes?\n",
    "verbindung publisher/creator - spatialClusterName; dazu: dateClean, century, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createGraph(consideredDataFrame):\n",
    "    # for testing purposes it is handy to limit the number of records\n",
    "    #consideredDataFrame=consideredDataFrame[(consideredDataFrame.Year>=1961) & (consideredDataFrame.Year<1975)]\n",
    "\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.Graph()\n",
    "\n",
    "    rowCount=0\n",
    "    seenCreators=[]\n",
    "    seenLocations=[]\n",
    "    for row in consideredDataFrame.iterrows():\n",
    "        rowCount=rowCount+1\n",
    "        #if rowCount%1000==0:\n",
    "        #    printLog(\"Processed %i rows...\"%rowCount)\n",
    "        ppn=row[1][\"PPN\"]\n",
    "        creator=str(row[1][\"publisher\"]).upper()\n",
    "        if creator==\"NAN\":\n",
    "            creator=str(row[1][\"creator\"]).upper()\n",
    "        if creator==\"NAN\":\n",
    "            creator=\"Unknown creator\"\n",
    "        \n",
    "        location=row[1][\"spatialClusterName\"]\n",
    "        if location==\"NAN\":\n",
    "            location=\"s. l.\"\n",
    "        elif not location:\n",
    "            location=\"s. l.\"\n",
    "        year=row[1][\"dateClean\"]\n",
    "        subject=row[1][\"subject\"]\n",
    "        \n",
    "        #if not creator in G.nodes():\n",
    "        #if not creator in seenCreators:\n",
    "        \n",
    "        G.add_node(creator)\n",
    "        if not creator==\"Unknown creator\":\n",
    "            # the name attribute will be helpful for D3.js visualizations\n",
    "            G.node[creator]['name'] = creator\n",
    "            G.node[creator]['year'] = year\n",
    "            G.node[creator]['type'] = \"creator\"\n",
    "            G.node[creator]['subject']=subject\n",
    "            G.node[creator]['century']=row[1][\"century\"]\n",
    "            seenCreators.append(creator)\n",
    "        else:\n",
    "            G.node[creator]['name'] = creator\n",
    "            G.node[creator]['year'] = year\n",
    "            G.node[creator]['type'] = \"no_creator\"\n",
    "        \n",
    "        #if not location in G.nodes():\n",
    "        #if not location in seenLocations:\n",
    "       \n",
    "        # the name attribute will be helpful for D3.js visualizations\n",
    "        if not location==\"s. l.\":\n",
    "            G.add_node(location)\n",
    "            G.node[location]['name'] = location\n",
    "            G.node[location]['year'] = year\n",
    "            G.node[location]['type'] = \"location\"\n",
    "            seenLocations.append(location)\n",
    "            G.add_edge(creator,location)\n",
    "        else:\n",
    "            pass\n",
    "        # if we would like to add the unknown location, uncomment the following lines and the comment the last line\n",
    "            #G.add_node(location)\n",
    "            #G.node[location]['name'] = location\n",
    "            #G.node[location]['year'] = year\n",
    "            #G.node[location]['type'] = \"no_location\"\n",
    "    \n",
    "        #print location\n",
    "        #print creator\n",
    "        #print \"* * *\"\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Creating graphs and additional data...\")\n",
    "\n",
    "graphsPerCentury=dict()\n",
    "ppnPerCentury=OrderedDict()\n",
    "for century in range(7,21): # we know that there are only media from the 7th century on\n",
    "    if century in grpCentury.groups:\n",
    "        centDF=grpCentury.get_group(century)\n",
    "        returnedGraph=createGraph(centDF)\n",
    "        graphsPerCentury[century]=returnedGraph\n",
    "        printLog(\"Graph for century %i built with %i nodes and %i edges.\"%(century, len(returnedGraph.nodes()),len(returnedGraph.edges())))\n",
    "        # export as GraphML, which can be read by Gephi\n",
    "        nx.write_gml(returnedGraph,\"graphs/century_\"+str(century)+\".gml\")\n",
    "        \n",
    "        # now save the PPNs\n",
    "        ppnPerCentury[century]=[]\n",
    "        for row in centDF.iterrows():\n",
    "            ppn=row[1][\"PPN\"]\n",
    "            ppnPerCentury[century].append(ppn)\n",
    "\n",
    "# create the full graph\n",
    "returnedGraph=createGraph(df4)\n",
    "printLog(\"Graph for all centuries built with %i nodes and %i edges.\"%(len(returnedGraph.nodes()),len(returnedGraph.edges())))\n",
    "nx.write_gml(returnedGraph,\"graphs/all_centuries.gml\")\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFullGraph(consideredDataFrame):\n",
    "    # for testing purposes it is handy to limit the number of records\n",
    "    #consideredDataFrame=consideredDataFrame[(consideredDataFrame.Year>=1961) & (consideredDataFrame.Year<1975)]\n",
    "\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.Graph()\n",
    "\n",
    "    rowCount=0\n",
    "    seenCreators=[]\n",
    "    seenLocations=[]\n",
    "    for row in consideredDataFrame.iterrows():\n",
    "        rowCount=rowCount+1\n",
    "        #if rowCount%1000==0:\n",
    "        #    printLog(\"Processed %i rows...\"%rowCount)\n",
    "        ppn=row[1][\"PPN\"]\n",
    "        creator=str(row[1][\"publisher\"]).upper()\n",
    "        if creator==\"NAN\":\n",
    "            creator=str(row[1][\"creator\"]).upper()\n",
    "        if creator==\"NAN\":\n",
    "            creator=\"Unknown creator\"\n",
    "        \n",
    "        location=row[1][\"spatialClusterName\"]\n",
    "        if location==\"NAN\":\n",
    "            location=\"s. l.\"\n",
    "        elif not location:\n",
    "            location=\"s. l.\"\n",
    "        year=row[1][\"dateClean\"]\n",
    "        subject=row[1][\"subject\"]\n",
    "        \n",
    "        #if not creator in G.nodes():\n",
    "        #if not creator in seenCreators:\n",
    "        \n",
    "        G.add_node(creator)\n",
    "        if not creator==\"Unknown creator\":\n",
    "            # the name attribute will be helpful for D3.js visualizations\n",
    "            G.node[creator]['name'] = creator\n",
    "            G.node[creator]['year'] = year\n",
    "            G.node[creator]['type'] = \"creator\"\n",
    "            G.node[creator]['subject']=subject\n",
    "            G.node[creator]['century']=row[1][\"century\"]\n",
    "            seenCreators.append(creator)\n",
    "        else:\n",
    "            G.node[creator]['name'] = creator\n",
    "            G.node[creator]['year'] = year\n",
    "            G.node[creator]['type'] = \"no_creator\"\n",
    "        \n",
    "        #if not location in G.nodes():\n",
    "        #if not location in seenLocations:\n",
    "       \n",
    "        # the name attribute will be helpful for D3.js visualizations\n",
    "        if not location==\"s. l.\":\n",
    "            G.add_node(location)\n",
    "            G.node[location]['name'] = location\n",
    "            G.node[location]['year'] = year\n",
    "            G.node[location]['type'] = \"location\"\n",
    "            seenLocations.append(location)\n",
    "            G.add_edge(creator,location)\n",
    "        else:\n",
    "            pass\n",
    "        # if we would like to add the unknown location, uncomment the following lines and the comment the last line\n",
    "            #G.add_node(location)\n",
    "            #G.node[location]['name'] = location\n",
    "            #G.node[location]['year'] = year\n",
    "            #G.node[location]['type'] = \"no_location\"\n",
    "    \n",
    "        #print location\n",
    "        #print creator\n",
    "        #print \"* * *\"\n",
    "        \n",
    "        G.add_node(ppn)\n",
    "        G.node[ppn]['name'] = str(ppn)\n",
    "        G.node[ppn]['year'] = year\n",
    "        G.node[ppn]['type'] = \"PPN\"\n",
    "        G.node[ppn]['century']=row[1][\"century\"]\n",
    "        G.add_edge(ppn,creator)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Creating graphs incl. PPNs...\")\n",
    "\n",
    "graphsPerCentury=dict()\n",
    "for century in range(7,21): # we know that there are only media from the 7th century on\n",
    "    if century in grpCentury.groups:\n",
    "        centDF=grpCentury.get_group(century)\n",
    "        returnedGraph=createFullGraph(centDF)\n",
    "        graphsPerCentury[century]=returnedGraph\n",
    "        gmlPath=\"graphs/century_ppn_\"+str(century)+\".gml\"\n",
    "        # export as GraphML, which can be read by Gephi\n",
    "        nx.write_gml(returnedGraph,gmlPath)\n",
    "        printLog(\"Graph for century %i built with %i nodes and %i edges (see %s).\"%(century, len(returnedGraph.nodes()),len(returnedGraph.edges()),gmlPath))\n",
    "        \n",
    "\n",
    "# create the full graph\n",
    "returnedGraph=createFullGraph(df4)\n",
    "gmlPath=\"graphs/all_centuries_ppn.gml\"\n",
    "nx.write_gml(returnedGraph,gmlPath)\n",
    "printLog(\"Graph for all centuries built with %i nodes and %i edges (see %s).\"%(len(returnedGraph.nodes()),len(returnedGraph.edges()),gmlPath))\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ca. 45 min for a graph built with 16368 nodes and 18539 edges on my MacBook Pro, i.e., with the full data set\n",
    "# with http://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.layout.spring_layout.html#networkx.drawing.layout.spring_layout\n",
    "# theory behind it: https://en.wikipedia.org/wiki/Force-directed_graph_drawing\n",
    "#printLog(\"Preparing to draw...\")\n",
    "\n",
    "#nx.draw(G)\n",
    "#plt.savefig(\"graph.pdf\")\n",
    "#printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generic graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenericGraph(consideredDataFrame):\n",
    "    # for testing purposes it is handy to limit the number of records\n",
    "    #consideredDataFrame=consideredDataFrame[(consideredDataFrame.Year>=1961) & (consideredDataFrame.Year<1975)]\n",
    "\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.Graph()\n",
    "\n",
    "    rowCount=0\n",
    "    seenCreators=[]\n",
    "    seenLocations=[]\n",
    "    # [u'PPN', u'alternative', u'creator', u'dataProvider', u'date', \n",
    "    # u'description', u'format', u'identifier', u'isShownAt', u'issued', \n",
    "    # u'object', u'provider', u'publisher', u'relation', u'rights', u'source', \n",
    "    # u'spatial', u'subject', u'title', u'type', u'spatialClean', u'dateClean', \n",
    "    # u'spatialCluster', u'spatialClusterName', u'latitude', u'longitude', u'century']\n",
    "    ignoreList=[u'rights', u'issued',u'source',u'spatial',u'dataProvider', u'date',u'format',u'spatialCluster',u'century', u'provider']\n",
    "    for row in consideredDataFrame.iterrows():#itertuples(): \n",
    "        ppn=val=row[1][\"PPN\"]\n",
    "        G.add_node(ppn)\n",
    "        G.node[ppn]['name'] = ppn\n",
    "        G.node[ppn]['type'] = \"PPN\"\n",
    "            \n",
    "        keys=row[1].keys()\n",
    "        for k in keys:\n",
    "            elementEmpty=False\n",
    "            if k not in ignoreList:\n",
    "                #print k\n",
    "                val=row[1][k]\n",
    "                if val: \n",
    "                    if type(val) is float:\n",
    "                        if not math.isnan(val):\n",
    "                            #print row[1][k]\n",
    "                            pass\n",
    "                        else:\n",
    "                            #print \"I DON'T KNOW!\"\n",
    "                            elementEmpty=True\n",
    "                    else:\n",
    "                        #print row[1][k]\n",
    "                        pass\n",
    "                else:\n",
    "                    #print \"I DON'T KNOW!\"\n",
    "                    elementEmpty=True\n",
    "                \n",
    "                # only process non-empty elements\n",
    "                if not elementEmpty:\n",
    "                    if not val in G.nodes():\n",
    "                        G.add_node(val)\n",
    "                        G.node[val]['name'] = val\n",
    "                        G.node[val]['type'] = k\n",
    "                    \n",
    "                    G.add_edge(ppn,val)\n",
    "            \n",
    "        # spatialClean auf spatialClusterName binden, den rest dann mit spatialClusterName\n",
    "        # spatialClusterName erhält als Attribut latitude/longitude\n",
    "\n",
    "       \n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if allowComputationallyExpensiveCalculations:\n",
    "    saveDir=\"./graphs/generics/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "    printLog(\"Creating graphs incl. PPNs...\")\n",
    "\n",
    "    graphsPerCentury=dict()\n",
    "    for century in range(7,21):#(7,21): # we know that there are only media from the 7th century on\n",
    "        if century in grpCentury.groups:\n",
    "            centDF=grpCentury.get_group(century)\n",
    "            returnedGraph=createGenericGraph(centDF)\n",
    "            graphsPerCentury[century]=returnedGraph\n",
    "            gmlPath=\"graphs/generics/full_\"+str(century)+\".gml\"\n",
    "            # export as GraphML, which can be read by Gephi\n",
    "            nx.write_gml(returnedGraph,gmlPath)\n",
    "            printLog(\"Graph for century %i built with %i nodes and %i edges (see %s).\"%(century, len(returnedGraph.nodes()),len(returnedGraph.edges()),gmlPath))\n",
    "    printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "Graph mit allen Städten und PPNs eines Spatial Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpCentury.get_group(17).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Collection by Visual Content\n",
    "* reading the feature files takes approx. 12 min\n",
    "* erklären, woher die CBIR features kommen (https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)\n",
    "\n",
    "![Visual Words](img/visword.jpg)\n",
    "By Masterwaw - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=19645418\n",
    "\n",
    "Scale-invariant feature transform, reference for local feature detection\n",
    "\n",
    "![Visual Word Generation](img/visword_generation2.png)\n",
    "\n",
    "int sampleDocumentsToCreateCodebook = 5000; \n",
    "int numberOfClusters = 1000;\n",
    "\n",
    "next step takes ca. 40 minutes if the raw features are loaded from disk and converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featureBaseDir=\"./featureFiles.5k1k/\"\n",
    "missingPPNs=[]\n",
    "readPPNs=[]\n",
    "featuresPPN=[]\n",
    "featsPerCentury=dict()\n",
    "readPpnPerCentury=dict()\n",
    "\n",
    "printLog(\"Loading features...\")\n",
    "if reinterpretVisualWordRawFeatures:\n",
    "    for century in range(7,21):\n",
    "        if century in grpCentury.groups:\n",
    "            featsPerCentury[century]=[]\n",
    "    for century in range(7,21):\n",
    "        if century in grpCentury.groups:\n",
    "            readPpnPerCentury[century]=[]\n",
    "\n",
    "    index=0\n",
    "    for row in df4.iterrows():\n",
    "        index=index+1\n",
    "        if index%10000==0:\n",
    "            printLog(\"Processed %i documents.\"%index)\n",
    "        ppn=str(row[1][\"PPN\"])\n",
    "        if os.path.isfile(featureBaseDir+ppn+\".csv\"):\n",
    "            #print ppn+\" okay.\"\n",
    "            featFile=open(featureBaseDir+ppn+\".csv\")\n",
    "            for line in featFile:\n",
    "                feature=line\n",
    "            tokens=feature.split()\n",
    "            harray=[]\n",
    "            for t in tokens:\n",
    "                harray.append(int(t,16))\n",
    "            featFile.close()\n",
    "\n",
    "            readPPNs.append(ppn)\n",
    "            featuresPPN.append(np.array(harray,dtype=np.uint8))\n",
    "            # check to which century the feature belongs\n",
    "            for century in range(7,21):\n",
    "                if century in grpCentury.groups:\n",
    "                    if ppn in ppnPerCentury[century]:\n",
    "                        readPpnPerCentury[century].append(ppn)\n",
    "                        featsPerCentury[century].append(np.array(harray,dtype=np.uint8))\n",
    "        else:\n",
    "            missingPPNs.append(ppn)\n",
    "    printLog(\"Done.\")\n",
    "    printLog(\"Number of missing PPNs: %i\"%len(missingPPNs))\n",
    "    \n",
    "    # pickling takes about 15 minutes\n",
    "    pickleCompress('./picklez/missingPPNs.picklez',missingPPNs)\n",
    "    pickleCompress('./picklez/readPPNs.picklez',readPPNs)\n",
    "    pickleCompress('./picklez/featuresPPN.picklez',featuresPPN)\n",
    "    pickleCompress('./picklez/featsPerCentury.picklez',featsPerCentury)\n",
    "    pickleCompress('./picklez/readPpnPerCentury.picklez',readPpnPerCentury)\n",
    "    printLog(\"Pickling completed.\")\n",
    "else:\n",
    "    # takes about 1 minute\n",
    "    missingPPNs=pickleDecompress('./picklez/missingPPNs.picklez')\n",
    "    readPPNs=pickleDecompress('./picklez/readPPNs.picklez')\n",
    "    featuresPPN=pickleDecompress('./picklez/featuresPPN.picklez')\n",
    "    featsPerCentury=pickleDecompress('./picklez/featsPerCentury.picklez')\n",
    "    readPpnPerCentury=pickleDecompress('./picklez/readPpnPerCentury.picklez')\n",
    "    printLog(\"Loading from disk completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering of 101031 elements started with 1000 as cluster target size: 3 min\n",
    "* note that the cluster labels vary for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with all features will most likely halt your computer because of the memory consumption if you use KMeans!\n",
    "feats=featuresPPN#[:20000] \n",
    "\n",
    "# define the number of clusters to be found\n",
    "true_k=1000\n",
    "printLog(\"Clustering of %i elements started with %i as cluster target size.\"%(len(feats),true_k))\n",
    "# initialize the k-means algorithm\n",
    "#km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "# we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "# apply the algorithm on the data\n",
    "km.fit(feats)\n",
    "printLog(\"Clustering finished.\")\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir=\"./html/_clusteroverview_allcents/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "imgDir=\"../../web/thumbnails/\"\n",
    "#imgDir=\"file:///Volumes/2TB_WD/sbb_images/tmp/\"\n",
    "htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "htmlTail=\"</body></html>\"\n",
    "clusters=dict()\n",
    "for i,val in enumerate(km.labels_):\n",
    "    if val not in clusters:\n",
    "        clusters[val]=[]\n",
    "    clusters[val].append(readPPNs[i])\n",
    "#print clusters\n",
    "for i in clusters:\n",
    "    htmlOut=open(saveDir+str(i)+\".html\",\"w\")\n",
    "    htmlOut.write(htmlHead+\"\\n\")\n",
    "    htmlOut.write(\"<a href='\"+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "    for ppn in clusters[i]:\n",
    "        htmlOut.write(\"<img width='170' src='\"+imgDir+ppn+\".jpg' />\\n\")\n",
    "    htmlOut.write(htmlTail)\n",
    "    htmlOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat the same step for each century..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clustersPerCentury=dict()\n",
    "maxClusterAmount=100\n",
    "printLog(\"Starting clustering per century...\")\n",
    "for century in featsPerCentury:\n",
    "    maxClusters=len(featsPerCentury[century])\n",
    "    # define the number of clusters to be found\n",
    "    true_k=int(maxClusters*0.1+1)\n",
    "    # restrict the number of clusters to prevent extremely large clusters\n",
    "    if true_k>(maxClusterAmount*2):\n",
    "        true_k=maxClusterAmount+int(maxClusters*0.005)\n",
    "    elif true_k>maxClusterAmount:\n",
    "        true_k=maxClusterAmount+int(maxClusters*0.01)\n",
    "    printLog(\"Clustering of %i element(s) started with %i as cluster target size for century %i.\"%(maxClusters,true_k,century))\n",
    "    # initialize the k-means algorithm\n",
    "    #km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    # we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "    # apply the algorithm on the data\n",
    "    km.fit(featsPerCentury[century])\n",
    "    clustersPerCentury[century]=km.labels_\n",
    "printLog(\"Clustering finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing 'centroids' takes up to 6 minutes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterCentroidsPerCentury=dict()\n",
    "numberCentroids=0\n",
    "\n",
    "printLog(\"Computing 'centroids' for...\")\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    printLog(\"\\tcentury %i\"%century)\n",
    "    clusters=dict()\n",
    "    centFeats=dict()\n",
    "\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in centFeats:\n",
    "            centFeats[val]=[]\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        index=readPPNs.index(readPpnPerCentury[century][i])\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "        centFeats[val].append(featuresPPN[index])\n",
    "\n",
    "    clusterCentroidsPerCentury[century]=dict()\n",
    "    for cluster in centFeats:\n",
    "        r=centFeats[cluster]\n",
    "        meanDistances=[]\n",
    "        D=pairwise_distances(r,r)\n",
    "        #A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None.\n",
    "        #If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.\n",
    "        # distance between element 0 and 13 (=0.0 if X and Y are anti-correlated)\n",
    "        #D[0][13]\n",
    "        for row in D:\n",
    "            # each row in D stands for one document and its distances to all other documents\n",
    "            # by calculating its mean, we compute how dissimilar this document is to all others\n",
    "            meanDistances.append(np.mean(row))\n",
    "        #print meanDistances\n",
    "        minVal=np.min(meanDistances)\n",
    "        index=meanDistances.index(minVal)\n",
    "        clusterCentroidsPerCentury[century][cluster]=clusters[cluster][index]\n",
    "        numberCentroids=numberCentroids+1\n",
    "        #print str(cluster)+\": \"+str(clusters[cluster][index])+\" (of %i elements)\"%len(meanDistances)\n",
    "\n",
    "printLog(\"Done computing %i 'centroids'.\"%numberCentroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* iterate over all centuries and save output per centroids\n",
    "* CSV output is for the visualization with the web-based QA tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saveDir=\"./html/_clusteroverview_per_century/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "printLog(\"Creating HTML and CSV output...\")\n",
    "\n",
    "# limits the shown PPNs per century-separated cluster\n",
    "limitClusterCentroidsPerCentury=10\n",
    "# for the cluster detail views limitClusterCentroidsPerCentury*centuryLimitFactor elements will be displayed per centroid\n",
    "centuryLimitFactor=3\n",
    "\n",
    "csvOut=open(\"./web/data/clusters.csv\",\"w\")\n",
    "csvOut.write(\"id,value\\n\")\n",
    "rootNode=\"all.\"\n",
    "csvOut.write(\"all,\"+\"\\n\")\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    largestClusterSize=0\n",
    "    largestCluster=None\n",
    "    \n",
    "    csvCenturyOut=open(\"./web/data/\"+str(century)+\".csv\",\"w\")\n",
    "    csvCenturyOut.write(\"id,value\\n\")\n",
    "    rootCenturyNode=\"all\"\n",
    "    csvCenturyOut.write(rootCenturyNode+\",\"+\"\\n\")\n",
    "    \n",
    "    csvOut.write(\"all.\"+str(century)+\",\"+\"\\n\")\n",
    "    \n",
    "    saveDir=\"./html/_clusteroverview_per_century/\"+str(century)+\"/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    imgDir=\"../../../web/thumbnails/\"\n",
    "    #imgDir=\"file:///Volumes/2TB_WD/sbb_images/tmp/\"\n",
    "    htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "    htmlTail=\"</body></html>\"\n",
    "    \n",
    "    clusters=dict()\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "        \n",
    "\n",
    "    clusterSizes=[]\n",
    "    shownCentroidCount=0\n",
    "    shownMoreCentroidsAvailable=False\n",
    "    \n",
    "    noClustersInCentury=len(clusters)\n",
    "    for i in clusters:\n",
    "        clusterSizes.append(len(clusters[i]))\n",
    "        \n",
    "        if largestClusterSize<len(clusters[i]):\n",
    "            largestClusterSize=len(clusters[i])\n",
    "            largestCluster=i\n",
    "        \n",
    "        # the cluster's centroid\n",
    "        # 14/14: PPN789774356\n",
    "        #<br/>\n",
    "        #<img src='file:///Users/david/Documents/src/python/CulturalAnalytics/tmp/PPN789774356.jpg' />\n",
    "        #print \"\\tCentroid for cluster \"+str(i)+\": \"+str(clusterCentroidsPerCentury[century][i])\n",
    "        centroid=\"<img src='\"+imgDir+str(clusterCentroidsPerCentury[century][i])+\".jpg' />\\n\"+\"<br/>\\n\"\n",
    "    \n",
    "        if shownCentroidCount<limitClusterCentroidsPerCentury:\n",
    "            shownCentroidCount=shownCentroidCount+1\n",
    "            csvOut.write(rootNode+str(century)+\".\"+str(clusterCentroidsPerCentury[century][i])+\",\\n\")\n",
    "        else:\n",
    "            if not shownMoreCentroidsAvailable:\n",
    "                csvOut.write(rootNode+str(century)+\".more,\\n\")\n",
    "                shownMoreCentroidsAvailable=True\n",
    "                \n",
    "        csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\",\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        htmlOut=open(saveDir+str(i)+\".html\",\"w\")\n",
    "        htmlOut.write(htmlHead+\"\\n\")\n",
    "        #htmlOut.write(\"<a href='\"+str(century)+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(century)+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "        htmlOut.write(\"<a href='\"+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "        \n",
    "        htmlOut.write(centroid)\n",
    "        \n",
    "        centroidPPN=str(clusterCentroidsPerCentury[century][i])#str(clusters[i][0])\n",
    "        csvPPNOut=open(\"./web/data/\"+centroidPPN+\".csv\",\"w\")\n",
    "        csvPPNOut.write(\"id,value\\n\")\n",
    "        rootPPNNode=\"all\"\n",
    "        csvPPNOut.write(rootPPNNode+\",\"+\"\\n\")\n",
    "        csvPPNOut.write(rootPPNNode+\".\"+centroidPPN+\",\\n\")\n",
    "        ppnCount=0\n",
    "    \n",
    "        for ppn in clusters[i]:\n",
    "            htmlOut.write(\"<img width='170' src='\"+imgDir+ppn+\".jpg' alt='\"+ppn+\"'/>\\n\")\n",
    "            #csvOut.write(rootNode+str(century)+\".\"+str(clusterCentroidsPerCentury[century][i])+\".\"+ppn+\",\\n\")\n",
    "            if ppnCount<limitClusterCentroidsPerCentury*centuryLimitFactor:\n",
    "                ppnCount=ppnCount+1\n",
    "                csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\".\"+ppn+\",\\n\")\n",
    "            else:\n",
    "                csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\".more,\\n\")\n",
    "                break\n",
    "            \n",
    "            csvPPNOut.write(rootPPNNode+\".\"+centroidPPN+\".\"+ppn+\",\\n\")\n",
    "        csvPPNOut.close()\n",
    "        htmlOut.write(htmlTail)\n",
    "        htmlOut.close()\n",
    "    \n",
    "    csvCenturyOut.close()\n",
    "    print(\"\\tLargest cluster for century %i is %i with %i elements.\"%(century,largestCluster,largestClusterSize))\n",
    "    print(\"\\t\\tNumber of clusters: %i\"%noClustersInCentury)\n",
    "    print(\"\\t\\tMean cluster size: %s\"%str(np.mean(clusterSizes)))\n",
    "    print(\"\\t\\tCluster size standard deviation: %s\"%str(np.std(clusterSizes)))\n",
    "    print(\"\\t\\tMin. cluster size: %s\"%str(np.amin(clusterSizes)))\n",
    "    print(\"\\t\\tMax. cluster size: %s\"%str(np.amax(clusterSizes)))\n",
    "csvOut.close()\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Visual Timeline of Publications\n",
    "plotting of the \"centroids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDir=\"../web/thumbnails/\"\n",
    "htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "htmlTail=\"</body></html>\"\n",
    "centroidPath=\"html/_centroids.html\"\n",
    "\n",
    "printLog(\"Saving centroid overview HTML page at: \"+centroidPath)\n",
    "htmlOut=open(centroidPath,\"w\")\n",
    "htmlOut.write(htmlHead)\n",
    "for century in clustersPerCentury:\n",
    "    htmlOut.write(\"<h1 style='color:white;'>\"+str(century)+\"</h1>\\n\")\n",
    "    for centroid in clusterCentroidsPerCentury[century]:\n",
    "        htmlOut.write(\"<img width='170' src='\"+imgDir+clusterCentroidsPerCentury[century][centroid]+\".jpg' />\\n\")\n",
    "htmlOut.write(htmlTail)\n",
    "htmlOut.close()\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also extend the idea and create a graph of the data\n",
    "to base a nice visualization of the clusters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgDir=\"./web/thumbnails/\"\n",
    "printLog(\"Creating overview graph...\")\n",
    "G=nx.Graph()\n",
    "\n",
    "lastCentury=\"7\"\n",
    "for century in clustersPerCentury: \n",
    "#for century in [7,10,11,12,13,14,15,16]:#range(12,15):\n",
    "    strCentury=str(century)\n",
    "    G.add_node(strCentury)\n",
    "    G.node[strCentury]['name'] = strCentury\n",
    "    G.node[strCentury]['type'] = \"century\"\n",
    "    \n",
    "    for centroid in clusterCentroidsPerCentury[century]:\n",
    "        ppn=str(clusterCentroidsPerCentury[century][centroid])\n",
    "        imagePath=imgDir.replace(\"file://\",\"\")+ppn+\".jpg\"\n",
    "        G.add_node(ppn)\n",
    "        G.node[ppn]['name'] = ppn\n",
    "        if os.path.isfile(imagePath):\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            G.node[ppn]['imagePath'] = ppn\n",
    "        else:\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            G.node[ppn]['imagePath'] = \"none\"\n",
    "        G.node[ppn]['title'] = ppnLookup[ppn]['title']\n",
    "        G.node[ppn]['creator'] = ppnLookup[ppn]['creator']\n",
    "        if ppnLookup[ppn]['spatialClusterName']:\n",
    "            G.node[ppn]['location'] = ppnLookup[ppn]['spatialClusterName']\n",
    "        G.node[ppn]['locationRaw'] =ppnLookup[ppn]['spatialRaw']\n",
    "        \n",
    "        G.node[ppn]['mediatype'] =ppnLookup[ppn]['mediatype']\n",
    "        G.node[ppn]['subject'] =ppnLookup[ppn]['subject']\n",
    "        G.node[ppn]['source'] =ppnLookup[ppn]['source']\n",
    "        G.node[ppn]['publisher'] =ppnLookup[ppn]['publisher']\n",
    "        G.node[ppn]['alternative'] =ppnLookup[ppn]['alternative']\n",
    "            \n",
    "        G.node[ppn][\"century\"]=century\n",
    "        G.node[ppn]['dateClean'] =ppnLookup[ppn]['dateClean']\n",
    "        G.node[ppn][\"cluster\"]=str(centroid)\n",
    "        G.node[ppn]['lat']=ppnLookup[ppn]['lat']\n",
    "        G.node[ppn]['lng']=ppnLookup[ppn]['lng']\n",
    "        G.node[ppn]['type'] = \"image\"\n",
    "        G.node[ppn]['textCluster'] =ppnLookup[ppn]['textCluster']\n",
    "        G.node[ppn]['creatorCluster'] =ppnLookup[ppn]['creatorCluster']\n",
    "        \n",
    "        G.add_edge(strCentury,ppn)\n",
    "    G.add_edge(strCentury,lastCentury)\n",
    "    lastCentury=strCentury\n",
    "    \n",
    "nx.write_gml(G,\"graphs/century.gml\")\n",
    "d = json_graph.node_link_data(G)\n",
    "jsonPath='./web/data/century.json'\n",
    "#jsonPath='./force/century_test.json'\n",
    "saveDir=\"./web/force/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "json.dump(d, open(jsonPath,'w'))\n",
    "printLog(\"Done (see %s).\"%jsonPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Creating cluster graph output...\")\n",
    "imgDir=\"./web/thumbnails/\"\n",
    "\n",
    "saveDir=\"./web/data/clusters/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    printLog(\"Processing century \"+str(century))\n",
    "    saveDir=\"./web/data/clusters/\"+str(century)+\"/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    clusters=dict()\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "\n",
    "    for i in clusters:\n",
    "        G=nx.Graph()\n",
    "        # the cluster's centroid\n",
    "        centroid=str(clusterCentroidsPerCentury[century][i])\n",
    "        G.add_node(centroid)\n",
    "        G.node[centroid]['type'] = \"centroid\"\n",
    "        \n",
    "        for ppn in clusters[i]:\n",
    "            imagePath=imgDir.replace(\"file://\",\"\")+ppn+\".jpg\"\n",
    "            #dateClean=str(df4[df4.PPN==ppn].iloc[-1]['dateClean'])\n",
    "            dateClean=ppnLookup[ppn][\"dateClean\"]\n",
    "            G.add_node(dateClean)\n",
    "            G.node[dateClean]['name'] = dateClean\n",
    "            G.node[dateClean]['type'] = \"dateClean\"\n",
    "            G.add_edge(centroid,dateClean)\n",
    "            \n",
    "            G.add_node(ppn)\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            if os.path.isfile(imagePath):\n",
    "                G.node[ppn]['name'] = ppn\n",
    "                G.node[ppn]['imagePath'] = ppn\n",
    "            else:\n",
    "                G.node[ppn]['name'] = ppn\n",
    "                G.node[ppn]['imagePath'] = \"none\"\n",
    "            #G.node[ppn]['title'] = df4[df4.PPN==ppn].iloc[-1]['title']\n",
    "            #G.node[ppn]['creator'] = str(df4[df4.PPN==ppn].iloc[-1]['creator'])\n",
    "            #G.node[ppn]['location'] = df4[df4.PPN==ppn].iloc[-1]['spatialClusterName']\n",
    "            G.node[ppn]['title'] = ppnLookup[ppn][\"title\"]\n",
    "            G.node[ppn]['creator'] = ppnLookup[ppn][\"creator\"]\n",
    "            if ppnLookup[ppn][\"spatialClusterName\"]:\n",
    "                G.node[ppn]['location'] = ppnLookup[ppn][\"spatialClusterName\"]\n",
    "            else:\n",
    "                G.node[ppn]['location']=\"none\"\n",
    "            G.node[ppn]['locationRaw'] =ppnLookup[ppn]['spatialRaw']\n",
    "            \n",
    "            G.node[ppn]['mediatype'] =ppnLookup[ppn]['mediatype']\n",
    "            G.node[ppn]['subject'] =ppnLookup[ppn]['subject']\n",
    "            G.node[ppn]['source'] =ppnLookup[ppn]['source']\n",
    "            G.node[ppn]['publisher'] =ppnLookup[ppn]['publisher']\n",
    "            G.node[ppn]['alternative'] =ppnLookup[ppn]['alternative']\n",
    "            G.node[ppn]['dateClean'] =ppnLookup[ppn]['dateClean']\n",
    "            G.node[ppn]['lat']=ppnLookup[ppn]['lat']\n",
    "            G.node[ppn]['lng']=ppnLookup[ppn]['lng']\n",
    "            \n",
    "            if ppn==centroid:\n",
    "                G.node[centroid]['type'] = \"centroid\"\n",
    "            else:\n",
    "                G.node[ppn]['type'] = \"image\"\n",
    "            \n",
    "            G.add_edge(dateClean,ppn)\n",
    "            \n",
    "        nx.write_gml(G,saveDir+str(i)+\".gml\")\n",
    "        d = json_graph.node_link_data(G)\n",
    "        jsonPath='./web/data/clusters/'+str(century)+'/'+str(i)+'.json'\n",
    "        json.dump(d, open(jsonPath,'w'))\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Extravaganza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points are latitude, longitude\n",
    "# Latitudes range from -90 to 90.\n",
    "# Longitudes range from -180 to 180\n",
    "newport_ri = (41.49008, -71.312796)\n",
    "cleveland_oh = (41.499498, -81.695391)\n",
    "test=(-95,-161)\n",
    "x=vincenty(newport_ri, test)\n",
    "print(x.kilometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "ppnPosition=[]\n",
    "ppnPositionLabels=[]\n",
    "ppnPositionSpatialClean=[]\n",
    "\n",
    "printLog(\"Extracting latitude and longitude...\")\n",
    "\n",
    "for row in df4.iterrows():\n",
    "    index=index+1\n",
    "    if index%10000==0:\n",
    "        printLog(\"Processed %i documents.\"%index)\n",
    "    ppn=str(row[1][\"PPN\"])\n",
    "    try:\n",
    "        lat=float(row[1][\"latitude\"])\n",
    "        lng=float(row[1][\"longitude\"])\n",
    "        spatialClean=row[1][\"spatialClean\"]\n",
    "        if math.isnan(lat) and math.isnan(lng):\n",
    "            pass\n",
    "        else:\n",
    "            ppnPositionLabels.append(ppn)\n",
    "            ppnPositionSpatialClean.append(spatialClean)\n",
    "            ppnPosition.append((lat,lng))\n",
    "    except TypeError:\n",
    "        #print(row[1][\"latitude\"])\n",
    "        pass\n",
    "    \n",
    "printLog(\"Found %i PPNs with coordinates.\" %len(ppnPositionLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Distribution of Geo-Spatial Coordinates')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.scatter(*zip(*ppnPosition),alpha=0.1)\n",
    "if saveFiguresAsPDF:\n",
    "    plt.savefig('./figures/sample.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Latitudes range from -90 to 90. -> x-Achse sollte 180 breit sein, Mitte ist 90\n",
    "* Longitudes range from -180 to 180 -> y-Achse sollte 360 breit sein, Mitte ist 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=ppnPosition\n",
    "\n",
    "# define the number of clusters to be found\n",
    "true_k=30\n",
    "printLog(\"Clustering of %i elements started with %i as cluster target size.\"%(len(feats),true_k))\n",
    "# initialize the k-means algorithm\n",
    "#km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "# we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "# apply the algorithm on the data\n",
    "km.fit(feats)\n",
    "printLog(\"Clustering finished.\")\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounding boxes aus http://boundingbox.klokantech.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with GeoJSON (http://geojson.org/) https://pypi.python.org/pypi/geojson and OpenLayers  http://openlayers.org/en/latest/doc/ http://openlayers.org/en/latest/apidoc/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions=[\"Europe\",\"Africa\",\"Asia\",\"Australia\",\"SouthAmerica\",\"NorthAmerica\"]\n",
    "regionBoundingBox=dict()\n",
    "#westlimit=-22.5; southlimit=33.6; eastlimit=58.4; northlimit=82.9\n",
    "regionBoundingBox[\"Europe\"]=[33.6,82.9,-22.5,58.4]\n",
    "#africa westlimit=-22.9; southlimit=-63.7; eastlimit=58.0; northlimit=37.2\n",
    "regionBoundingBox[\"Africa\"]=[-63.7,37.2,-22.9,58.0]\n",
    "#asia westlimit=29.5; southlimit=-11.4; eastlimit=-168.4; northlimit=81.2\n",
    "regionBoundingBox[\"Asia\"]=[-11.4,81.2,-168.4,29.5]\n",
    "#australia westlimit=112.5; southlimit=-50.4; eastlimit=-162.9; northlimit=-10.0\n",
    "regionBoundingBox[\"Australia\"]=[-50.4,-10.0,-162.9,112.5]\n",
    "#south america westlimit=-119.5; southlimit=-57.0; eastlimit=-29.9; northlimit=28.1\n",
    "regionBoundingBox[\"SouthAmerica\"]=[-57.0,28.1,-119.5,-29.9]\n",
    "#north america westlimit=-169.1; southlimit=23.7; eastlimit=-20.0; northlimit=71.4\n",
    "regionBoundingBox[\"NorthAmerica\"]=[23.7,71.4,-169.1,-20.0]\n",
    "\n",
    "\n",
    "# man muss checken in welchen intervallen die kontinente liegen, insb. bei 2 und 3\n",
    "def getRegion(lat,lng):\n",
    "    for region in regions:\n",
    "        if region in regionBoundingBox:\n",
    "            if regionBoundingBox[region][0] <= lat <= regionBoundingBox[region][1]:\n",
    "                if regionBoundingBox[region][2] <= lng <= regionBoundingBox[region][3]:\n",
    "                    return region\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "#print getRegion(51,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all arrays and check if there is more than one location associated with a PPN\n",
    "# ppnPosition is (lat,lng)\n",
    "printLog(\"Creating GeoJSON data...\")\n",
    "multipleCitiesPPN=pickleDecompress(\"./picklez/multipleSpatialNamesPerPPN.picklez\")\n",
    "\n",
    "baseDir=\"./web/data/layers/\"\n",
    "fileName=baseDir+\"test.json\"\n",
    "locationFeatures=dict()\n",
    "locationFeaturesGlobal=[]\n",
    "\n",
    "spots=zip(ppnPositionLabels, ppnPositionSpatialClean,ppnPosition)\n",
    "\n",
    "for spot in spots:\n",
    "    ppn=spot[0]\n",
    "    leadingLoc=spot[1]\n",
    "    latLng=spot[2]\n",
    "    # GeoJSON points are in longitude , latitude but our storage is lat/lng\n",
    "    my_point = gj.Point((latLng[1], latLng[0]))\n",
    "\n",
    "    region=getRegion(latLng[0],latLng[1])\n",
    "    if region not in locationFeatures:\n",
    "        locationFeatures[region]=[]\n",
    "        print(\"Adding \"+region)\n",
    "    else:\n",
    "        locationFeatures[region].append(gj.Feature(geometry=my_point, properties={\"title\": leadingLoc+\" \"+ppn, \"ppn\":ppn}))\n",
    "    \n",
    "    locationFeaturesGlobal.append(gj.Feature(geometry=my_point, properties={\"title\": leadingLoc+\" \"+ppn, \"ppn\":ppn}))\n",
    "    #print ppn\n",
    "    if ppn in multipleCitiesPPN:\n",
    "        array=multipleCitiesPPN[ppn]\n",
    "        #if there are alternative locations for this PPN\n",
    "        if len(array)>1:\n",
    "            #print leadingLoc+\" \"+ppn\n",
    "            #print \"\\t\"+\"; \".join(array).encode(\"utf-8\")\n",
    "            pass\n",
    "\n",
    "for region in locationFeatures:\n",
    "    geoJSON_collection=gj.FeatureCollection(locationFeatures[region])\n",
    "    dump = gj.dumps(geoJSON_collection, sort_keys=True)\n",
    "\n",
    "    jsonFile = open(baseDir+region.lower()+\".json\", \"w\")\n",
    "    jsonFile.write(dump)\n",
    "    jsonFile.close()\n",
    "    \n",
    "\n",
    "geoJSON_collection=gj.FeatureCollection(locationFeaturesGlobal)\n",
    "dump = gj.dumps(geoJSON_collection, sort_keys=True)\n",
    "\n",
    "globalJSONPath=baseDir+\"global.json\"\n",
    "jsonFile = open(globalJSONPath, \"w\")\n",
    "jsonFile.write(dump)\n",
    "jsonFile.close()\n",
    "printLog(\"Serialized %i metadata records.\\n\\tSaved global JSON document at %s.\" %(len(locationFeaturesGlobal),globalJSONPath))\n",
    "\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print spots.index(\"PPN766441857\")\n",
    "#print(spots[10])\n",
    "#print(getRegion(spots[10][2][0],spots[10][2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create an inverted mapping from alternative location names to \"leading\" location names\n",
    "# the resulting dicts \"main\" key will be the leading character of the alternative location name \n",
    "# within this map will be a mapping to the \"leading\" location name, e.g., 'Corfu' would be mapped to 'Korfu'\n",
    "# lastLetter=r[0].lower()\n",
    "osmNames=pickleDecompress('./picklez/osm_names.picklez')\n",
    "osmAlternativesSorted=dict()\n",
    "for leadLoc in osmNames:\n",
    "    #print leadLoc\n",
    "    for v in osmNames[leadLoc].values():\n",
    "        leadingLetter=v[0].lower()\n",
    "        if not leadingLetter in osmAlternativesSorted:\n",
    "            osmAlternativesSorted[leadingLetter]=dict()\n",
    "        else:\n",
    "            osmAlternativesSorted[leadingLetter][v]=leadLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osmNames(key): alle alternativen titel in der Form names[loc][u'name']:\n",
    "# u'Ems': {u'name:nl': u'Eems', u'name': u'Ems', u'name:de': u'Ems', u'name:la': u'Amisia'...\n",
    "# latLng(key)[lat|lng]: latitude/longitude pro key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ideen\n",
    "\n",
    "* Timeline und Grafisches Aussehen, x-Achse: Zeit, y-Achse. Farbe? Brightness? Entropy? Abweichung vom Referenzbild (Distanz zum QBE)? https://www.slideshare.net/formalist/how-and-why-study-big-cultural-data-v2-15552598 #43\n",
    "* Dominante Farbe bestimmen, als 3D-Punkt nehmen und schauen, ob es mit etwas korreliert?\n",
    "* Stabi-URL:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g=df4.groupby(\"publisher\")\n",
    "#g.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.count().plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Service Functions\n",
    "to avoid problems with JSON access you should load the web pages from your own HTTP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:#launchHTTPServer:\n",
    "    # the resulting HTTP service will listen on port 8000 and open the main page in the browser\n",
    "    import http_server\n",
    "    http_server.load_url('web/webapps/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
